---
title: "Analysis for: Satellites reveal global extent of forced labor in the worldâ€™s fishing fleet"
author: "Gavin McDonald - Environmental Market Solutions Lab (emLab)"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: 
  pdf_document: 
    toc: yes
editor_options: 
  chunk_output_type: console
params:
     run_analysis: FALSE
     run_robustness: FALSE
     generate_figures: TRUE
     use_big_query: FALSE
---

```{r echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.height=7.5, fig.width=7.5,eval=FALSE,dev = 'png',dpi = 300)

## A note about using this R Markdown notebook:
## This is a parameterized notebook. The code chunks that will be run are determined by several toggles at the top of the document.
## run_analysis - If this is TRUE, the entire model cross-validation, training, and prediction analysis will be run. When this happens, all results will be cached and saved. This GitHub currently has cached results from the analysis as it was published in PNAS.
## run_robustness - If this is TRUE, the full suite of robustness checks will be run. When this happens, all results will be cached and saved. This GitHub currently has cached results from the analysis as it was published in PNAS.
## generate_figures - If this is TRUE, all figures for the PNAS manuscript will be generated. This not only generates figures for this markdown notebook, but also caches the figures as pdf files.This GitHub currently has cached figures as they were published in PNAS.
## use_big_query - In this section, we upload vessel-level risk predictions from the analysis to Google BigQuery. Using this information and matching it with spatial fishing effort and port visit information from Global Fishing Watch, we then download spatial forced labor risk fishing effort data and forced labor risk port visit data. **Note:** This section can only be run with proper authentication credentials for accessing specific tables in Global Fishing Watch's BigQuery project.
```

```{r eval = TRUE}
set.seed(101)
# Load Necessary libraries
library(tidyverse)
library(raster)
library(tidymodels)
library(ranger)
library(kernlab)
library(tictoc)
library(workflows)
library(countrycode)
library(bigrquery)
library(sf)
library(here)
library(rworldmap)
library(ggforce)
library(anonymizer)
library(cowplot)
library(kableExtra)
```

# Analysis

## Load data and necessary functions

```{r eval = (params$run_analysis | params$generate_figures | params$run_robustness)}
# Load custom threshold_perf function, adapted from probably::threshold_perf to give different metrics
source(here::here("r/threshold_perf_custom.R"))

# Set data directory based on system
# This references the emLab shared Team Drive
# This directory will be used for cacheing large results, as well as results from portions of the analysis that take a long time to run
# Note that for this analysis to be completely re-run, the user should specify an appropriate data directory. Otherwise, the most important cached data for reproducing the figures and statistics in the paper can be found within the GitHub repo itself.
data_directory <- ifelse(Sys.info()["sysname"]=="Windows","G:/Shared\ drives/emLab/Projects/current-projects/forced-labor-and-fisheries/data/phase-1",
                         "/Volumes/GoogleDrive/Shared\ drives/emLab/Projects/current-projects/forced-labor-and-fisheries/data/phase-1")

# Read in cached data
main_df <- read_csv(here::here("raw_data/s1_training_final.csv")) %>%
  filter(!is.na(flag)) %>%
  # Only include gears for which we have known offenders
  filter(gear %in% c("drifting_longlines","squid_jigger","trawlers")) %>%
  # Make binaries into 1s/0s
  mutate(offender = ifelse(offender,1,0),
         foc = ifelse(foc,1,0),
         iuu = ifelse(iuu,1,0)) 

filtered_main_df <- main_df %>%
  # Only only include vessel-years from positives in the year prior to being caught
  # Otherwise exclude them from training - they have potential for being positive, but we are not sure
  filter((offender == 1 & years_until_caught == 1) | offender == 0)  %>%
  dplyr::select(-years_until_caught)

training_df <- filtered_main_df %>%
  # Add media source id
  # If it's not a known offender, create dummy ID for CV
  mutate(source_id = ifelse(offender != 1,
                            paste0("no_source_",row_number()),
                            source_id)) %>%
  # Add forced labor database vessel id
  # If it's not a known offender, create dummy ID for CV
  mutate(fldb_vessel_id = ifelse(offender != 1,
                            paste0("no_fldb_info_",row_number()),
                            fldb_vessel_id)) %>%
  # Make these columns factors
  mutate_at(
     vars("foc", "iuu", "gear", "flag", "year", "ais_type", "mmsi", "fldb_vessel_id", "year", "source_id", "offender"),
     list(as.factor)) 

prediction_df <- main_df %>%
  mutate(offender = case_when(offender == 1 & years_until_caught == 1 ~ 1,
                              is.na(offender) ~ 0,
                              TRUE ~ 0)) %>%
  dplyr::select(-years_until_caught) %>%
  # Make these columns factors
  mutate_at(
     vars("foc", "iuu", "gear", "flag", "year", "ais_type", "mmsi", "fldb_vessel_id", "year", "source_id", "offender"),
     list(as.factor))

# Read in feature lookup table
feature_lookup <- read_csv(here::here("raw_data/s2_feature_lookup.csv"))
```

## Define pre-processing steps  

```{r eval = params$run_analysis}
# Recipe for processing data within each sampling iteration
fl_rec <- recipes::recipe(offender ~ ., data = head(training_df)) %>%
  recipes::update_role(mmsi, new_role = "id variable") %>%
  recipes::update_role(fldb_vessel_id, new_role = "id variable") %>%
  recipes::update_role(source_id, new_role = "id variable") %>%
  # Downsample unlabaled vessels
  # Skip=TRUE so that the testing dataset is not downsampled when baking
  # Do this at random every time
  #recipes::step_downsample("offender",under_ratio = tune(),skip=TRUE)  %>%
  recipes::step_knnimpute(ais_type,impute_with = c("gear","flag","length_m")) %>%
  # Box-Cox transformation on all numeric
  recipes::step_BoxCox(all_numeric()) %>%
  # Group flags with less than 1% into other
  recipes::step_other(flag, threshold = 0.01) %>%
  # Create dummy variables for factor columns like flag, ais_type
  recipes::step_dummy(all_predictors(), -all_numeric()) %>%
  # Remove near-zero variance numeric predictors
  recipes::step_nzv(all_predictors()) %>%
  # Remove numeric predictors that have correlation greater the 75%
  recipes::step_corr(all_numeric(), threshold = 0.75) %>%
  # Center all numeric predictors
  recipes::step_center(all_numeric()) %>%
  # Scale all numeric predictors
  recipes::step_scale(all_numeric())
```

## Specify models

```{r eval = params$run_analysis}
# Define model specs
# Random forest
# Use default hyperparameters
rf_mod <- rand_forest(
  mode = "classification",
  trees = 1000
) %>%
  set_engine("ranger")

# SVM
# Use default hyperparameters
svm_mod <- svm_rbf(mode = "classification") %>%
  set_engine("kernlab",scaled=FALSE)
```

## Cross-validation

```{r eval = params$run_analysis}
# Cross-validation
# Ensure there is no splitting across source_id (and consequently fldb_vessel_id) across analysis and assessment datasets
set.seed(101)
cv_splits <- group_vfold_cv(training_df, 
                            group = source_id,
                            v = 10)

# NA means no downsampling, so just the regular base classifier
under_ratio_vec <- tibble(under_ratio = c(NA,seq(1,5)))
# Number of bags
bag_vec <- tibble(bag = seq(100))
# Different models to try
model_type_vec <- tibble(model_type = c("random-forest","svm"))

analysis_data <- cv_splits %>%
  mutate(# Create analysis dataset based on CV folds
    analysis = map(splits,~analysis(.x)),
    # Create assessment dataset based on CV folds
    assessment = map(splits,~assessment(.x))) %>%
  dplyr::select(-splits)

model_runs <- crossing(under_ratio_vec,
                       model_type_vec,
           bag_vec) %>%
  # If there's no downsampling, there's no bagging
  mutate(bagging = ifelse(is.na(under_ratio),FALSE,TRUE)) %>%
  # Don't need to bag if not doing downsampling
  filter(!(!bagging & bag >1)) %>%
  # Want different seed each time downsampling is done, for bagging
  mutate(seed = sample.int(10^5,n()))

pb <- progress_estimated(nrow(model_runs) * nrow(cv_splits))

predictions <- model_runs %>%
  mutate(recipe = map2(seed,bagging,function(x,y){
    # Add downsampling to pre-processing recipe if bagging
    ifelse(!y,
           tmp_recipe <- fl_rec,
           tmp_recipe <- fl_rec %>%
             step_downsample("offender",under_ratio=y,seed=x,skip=TRUE))
    return(tmp_recipe)
    # Create workflow based on random forest hyperparameter and downsampling pre-processing recipe
  }),
  workflow = map2(recipe,model_type,function(x,y){
    # Add either random forest or svm to worflow, along with appropriate recipe
    ifelse(y == "random-forest",
           tmp_workflow <- workflow() %>%
             add_model(rf_mod) %>%
             add_recipe(x),
           tmp_workflow <- workflow() %>%
             add_model(svm_mod) %>%
             add_recipe(x))
    return(tmp_workflow)
  })) %>%
  # Remove unnecessary columns
  dplyr::select(-seed,-recipe) %>%
  # For each workflow, do entire model fitting process across all folds
  mutate(predict = map(workflow,function(x){
    analysis_data %>%
      mutate(predict = map2(analysis,assessment,function(y,z){
        tmp <- fit(x,y) %>%
          # Predict assessment data using fit
          predict(z, type = "prob") %>%
          # Add predictions to assessment data
          bind_cols(z) %>%
          # Select relevant columns
          dplyr::select(mmsi,year,offender,.pred_1)
        # Increment progress bar
        pb$tick()$print()
        return(tmp)
      })) %>%
      dplyr::select(id,predict) %>%
      unnest(predict)
  })) %>%
  dplyr::select(-workflow) %>%
  unnest(predict)

# Cache CV results
write_csv(predictions,paste0(data_directory,"/tidy_cv_predictions.csv"))
```

## Cross-validation performance

```{r eval = params$run_analysis}
# Load cached CV results
predictions <- read_csv(paste0(data_directory,"/tidy_cv_predictions.csv"))
bags_to_show <- c(1,10,50,100)
bag_performance <- map_df(bags_to_show,function(x){
  tmp_performance <- predictions %>%
    filter(bag<=x) %>%
    # For each fold and hyperparameter set, take average prediction for each vessel-year (mmsi and year) across bags
    group_by(id,model_type,under_ratio,mmsi,year) %>%
    summarize(offender = offender[1],
              .pred_1 = mean(.pred_1,na.rm=TRUE)) %>%
    ungroup()  %>%
  # Make offender =1 reference level
  mutate(offender = relevel(as.factor(offender),"1")) %>%
    # Then we will calculate performance metrics, by threshold, for each fold and hyperparameter set
    group_by(id,model_type,under_ratio) %>%
    nest() %>%
    ungroup() %>%
    # Calculated metrics will include recall, modified_f1, and detection_prevalence
    mutate(threshold_perf = map(data,~threshold_perf_custom(.x,
                                                            truth = offender,
                                                            estimate = .pred_1,
                                                            thresholds=seq(0.01,0.99,0.01)))) %>%
    dplyr::select(-data) %>%
    unnest(threshold_perf) %>%
    group_by(model_type,under_ratio,.threshold,.metric) %>%
          # Get mean, min, and SD of performance across folds
    # Don't compute if fold contains NAs or NaNs
    summarize(mean_performance = mean(.estimate),
              median_performance = median(.estimate),
              sd_performance = sd(.estimate),
              min_performance = min(.estimate)) %>%
    ungroup() %>%
    group_by(model_type,under_ratio) %>%
    nest() %>%
    ungroup() %>%
    # Now we will find the optimial threshold for maximizing the modified F1 estimator
    # Maximize the mean, while ensuring min performance across folds is at least 0
    mutate(optimal_threshold = map2_dbl(data,under_ratio,~.x %>%
                                          filter(min_performance >0 | is.na(.y)) %>%
                                         filter(.metric=="modified_f1") %>%
                                         arrange(desc(mean_performance)) %>%
                                         slice(1) %>%
                                         .$.threshold),
           # optimized_metrics will contain metrics with optimized modified F1 estimator
           optimized_metrics = map2(data,optimal_threshold,~.x %>%
                                      filter(.threshold==.y))) %>% 
    unnest(optimized_metrics) %>%
      dplyr::select(-optimal_threshold,-data) %>%
      mutate(number_bags = x)
    print(paste(x,"complete"))
    tmp_performance
  })
# Save bag performance estimates for later
write_csv(bag_performance,paste0(data_directory,"/tidy_cv_bag_performance.csv"))
write_csv(bag_performance,here::here("interim_data/s7_figure_s3_data.csv"))
```

## Choose optimized model

```{r eval = TRUE}
# Read in cached bag performance
bag_performance <- read_csv(here::here("interim_data/s7_figure_s3_data.csv"))

# Here we choose the optimzed model
# Non-bagging version has standard error too large
# Stick with 5 bags to reduce variation
# Stick with under_ratio of 1 for simplest model
# Random forest has lowest standard deviation for modified f1, and has a higher recall
optimized_model <- tibble(model_type = "random-forest",
                          bagging = TRUE,
                          under_ratio = 1,
                          number_bags = 100)

cv_performance <- bag_performance %>% 
  mutate(bagging = ifelse(is.na(under_ratio),FALSE,TRUE)) %>%
  inner_join(optimized_model,by = c("model_type", "number_bags", "under_ratio","bagging"))

optimized_model$.threshold <- cv_performance$.threshold[1]
```

## Final model building using full dataset and optimized model

```{r eval = params$run_analysis}
# Train final model on full dataset
set.seed(101)
# Define all model runs
model_runs_final <- optimized_model %>%
  crossing(tibble(bag = seq(optimized_model$number_bags))) %>%
  mutate(seed = sample.int(10^5,n()))

pb <- progress_estimated(nrow(model_runs_final))

predictions_final <- model_runs_final %>%
  mutate(recipe = map2(seed,bagging,function(x,y){
    # Add downsampling to pre-processing recipe if bagging
    ifelse(!y,
           tmp_recipe <- fl_rec,
           tmp_recipe <- fl_rec %>%
             step_downsample("offender",under_ratio=y,seed=x,skip=TRUE))
    return(tmp_recipe)
    # Create workflow based on random forest hyperparameter and downsampling pre-processing recipe
  }),
  workflow = map2(recipe,model_type,function(x,y){
    # Add either random forest or svm to worflow, along with appropriate recipe
    ifelse(y == "random-forest",
           tmp_workflow <- workflow() %>%
             add_model(rf_mod %>%
             set_args(importance = "impurity_corrected")) %>%
             add_recipe(x),
           tmp_workflow <- workflow() %>%
             add_model(svm_mod) %>%
             add_recipe(x))
    return(tmp_workflow)
  })) %>%
  # Remove unnecessary columns
  dplyr::select(-seed,-recipe) %>%
  # For each workflow, do entire model fitting process across all folds
  mutate(fit = map(workflow,~fit(.x,training_df)),
         predict = map(fit,function(x){
           tmp <- x %>%
             # Predict assessment data using fit
             predict(prediction_df, type = "prob") %>%
             # Add predictions to assessment data
             bind_cols(prediction_df)%>%
             # Select relevant columns
             dplyr::select(mmsi,year,offender,.pred_1)
           # Increment progress bar
           pb$tick()$print()
           return(tmp)
         })) %>%
  dplyr::select(-workflow)

saveRDS(predictions_final,paste0(data_directory,"/predictions_final.Rdata"))

as.numeric.factor <- function(x) {as.numeric(levels(x))[x]}
predictions_figures <- predictions_final %>%  
  dplyr::select(-fit) %>%
  unnest(predict) %>%
  # Switch factors to numeric for faster computation, then switch back
  mutate(offender = as.numeric.factor(offender),
         mmsi = as.numeric.factor(mmsi),
         year = as.numeric.factor(year)) %>%
    # Take average prediction for each vessel-year (mmsi and year) across bags
    group_by(mmsi,year) %>%
    summarize(offender = offender[1],
              .pred_1 = mean(.pred_1,na.rm=TRUE)) %>%
    ungroup() %>% 
  mutate(mmsi = as.factor(mmsi),
         year = as.factor(year)) %>%
  left_join(prediction_df %>%
              dplyr::select(-offender),
            by=c("mmsi","year")) %>%
  mutate(Country = countrycode(flag,"iso3c","country.name"))%>%
  # Use optimal threshold found above
  mutate(class = ifelse(.pred_1 >optimized_model$.threshold,1,0))%>% 
  mutate(Prediction = ifelse(class == 1,"Positive","Negative")) %>%
  mutate(Label = ifelse(offender == 0,"Unlabeled","Positive"))

# Save final predictions with MMSI
# This will late be uploaded to BQ, and MMSI will be used to match vessel with fishing activity and port visits
write_csv(predictions_figures,here::here("interim_data/s4_final_model_predictions_private.csv"))
# Save final predictions with anonymized MMSI, for public release
write_csv(predictions_figures %>%
            mutate(mmsi_anonymous = anonymize(mmsi),.seed=101) %>%
            dplyr::select(-mmsi),here::here("interim_data/s4_final_model_predictions.csv"))

# Select MMSI that are associated with an fldb vessel in its most recent year of operation
fldb_mmsi <- main_df %>% 
  filter(!is.na(fldb_vessel_id)) %>%
  group_by(fldb_vessel_id) %>%
  arrange(desc(year)) %>%
  slice(1) %>%
  ungroup() %>%
  distinct(mmsi,fldb_vessel_id) %>%
  mutate(mmsi = as.factor(mmsi))

# Get predictions for all those MMSI across the whole time series
predictions_fldb_mmsi <- predictions_figures %>% 
  dplyr::select(year,class,mmsi,offender) %>%
  inner_join(fldb_mmsi,by="mmsi")

# Save data for figure S1
write_csv(predictions_fldb_mmsi,here::here("interim_data/s12_figure_s1_data.csv"))


# Get average variable importance across bags

importance_data <- predictions_final %>%
  mutate(var_imp = map(fit,function(x){
    x$fit$fit$fit$variable.importance%>%
          as.data.frame() %>%
          rownames_to_column() %>%
          `colnames<-`(c("Feature", "Importance"))
  })) %>%
  dplyr::select(bag,var_imp) %>%
  unnest(var_imp) %>%
  group_by(Feature) %>%
  summarize(Importance=mean(Importance,na.rm=TRUE)) %>%
  ungroup() %>%
  left_join(feature_lookup %>%
              rename(Feature = indicator),by="Feature") %>%
  mutate(indicator_name = case_when(Feature == "gear_squid_jigger" ~ "Squid jigger",
                                    Feature == "gear_drifting_longlines" ~ "Longliner",
                                    Feature == "gear_trawlers" ~ "Trawler",
                                    Feature == "ais_type_A" ~ "AIS device type A",
                                    Feature == "ais_type_B" ~ "AIS device type B",
                                    Feature == "flag_other" ~ "Flag: Other",
                                    str_detect(Feature,"year") ~ paste0("Year: ", str_extract(Feature,"(\\d)+")),
                                    str_detect(Feature,"flag") ~ paste0("Flag: ", countrycode(word(Feature, 2, sep = "_"),"iso3c","country.name")),
                                    TRUE ~ indicator_name)) 
# Save data for figure 4
write_csv(importance_data,here::here("interim_data/s8_figure_s5_data.csv"))
```

\pagebreak

# Robustness checks

```{r eval = params$run_robustness}
robustness_results_df <- tibble(year_assumption = sort(unique(main_df$years_until_caught))) %>% 
  crossing(tibble(include_vessel_characteristics = c(TRUE,FALSE))) %>%
  mutate(robustness_results = map2(year_assumption,
                                   include_vessel_characteristics,
         function(year_assumption,include_vessel_characteristics){


  if(is.na(year_assumption)) return()

  filtered_main_df <- main_df %>%
  # Only only include vessel-years from positives in the year prior to being caught
  # Otherwise exclude them from training - they have high potential for being positive
  #dplyr::select(-fldb_vessel_id,-offender) %>%
  filter((offender == 1 & years_until_caught <= year_assumption) | offender == 0)  %>%
  dplyr::select(-years_until_caught)

training_df <- filtered_main_df %>%
  # Add media source id
  # If it's not a known offender, create dummy ID for CV
  mutate(source_id = ifelse(offender != 1,
                            paste0("no_source_",row_number()),
                            source_id)) %>%
  # Add forced labor database vessel id
  # If it's not a known offender, create dummy ID for CV
  mutate(fldb_vessel_id = ifelse(offender != 1,
                            paste0("no_fldb_info_",row_number()),
                            fldb_vessel_id)) %>%
  # Make these columns factors
  mutate_at(
     vars("foc", "iuu", "gear", "flag", "year", "ais_type", "mmsi", "fldb_vessel_id", "year", "source_id", "offender"),
     list(as.factor)) 

prediction_df <- main_df %>%
  mutate(offender = case_when(offender == 1 & years_until_caught <= year_assumption ~ 1,
                              is.na(offender) ~ 0,
                              TRUE ~ 0)) %>%
  dplyr::select(-years_until_caught) %>%
  # Make these columns factors
  mutate_at(
     vars("foc", "iuu", "gear", "flag", "year", "ais_type", "mmsi", "fldb_vessel_id", "year", "source_id", "offender"),
     list(as.factor))

  fl_rec <- recipes::recipe(offender ~ ., data = head(training_df)) %>%
    recipes::update_role(mmsi, new_role = "id variable") %>%
    recipes::update_role(fldb_vessel_id, new_role = "id variable") %>%
    recipes::update_role(source_id, new_role = "id variable") %>%
    # Downsample unlabaled vessels
    # Skip=TRUE so that the testing dataset is not downsampled when baking
    # Do this at random every time
    #recipes::step_downsample("offender",under_ratio = tune(),skip=TRUE)  %>%
    recipes::step_knnimpute(ais_type,impute_with = c("gear","flag","length_m")) %>%
    # Box-Cox transformation on all numeric
    recipes::step_BoxCox(all_numeric()) %>%
    # Group flags with less than 1% into other
    recipes::step_other(flag, threshold = 0.01) %>%
    # Create dummy variables for factor columns like flag, ais_type
    recipes::step_dummy(all_predictors(), -all_numeric()) %>%
    # Remove near-zero variance numeric predictors
    recipes::step_nzv(all_predictors()) %>%
    # Remove numeric predictors that have correlation greater the 75%
    recipes::step_corr(all_numeric(), threshold = 0.75) %>%
    # Center all numeric predictors
    recipes::step_center(all_numeric()) %>%
    # Scale all numeric predictors
    recipes::step_scale(all_numeric())
  
  if(!include_vessel_characteristics) fl_rec <- fl_rec %>%
    recipes::update_role(engine_power_kw,tonnage_gt,length_m,crew_size,ais_type, new_role = "id variable")

  rf_mod <- rand_forest(
    mode = "classification",
    trees = 1000
  ) %>%
    set_engine("ranger")
  
  # Cross-validation
  # Ensure there is no splitting across source_id (and consequently fldb_vessel_id) across analysis and assessment datasets
  set.seed(101)
  cv_splits <- group_vfold_cv(training_df, 
                              group = source_id,
                              v = 10)
  
  # NA means no downsampling, so just the regular base classifier
  under_ratio_vec <- tibble(under_ratio = 1)
  # Number of bags
  bag_vec <- tibble(bag = seq(100))
  # Different models to try
  model_type_vec <- tibble(model_type = c("random_forest"))
  
  analysis_data <- cv_splits %>%
    mutate(# Create analysis dataset based on CV folds
      analysis = map(splits,~analysis(.x)),
      # Create assessment dataset based on CV folds
      assessment = map(splits,~assessment(.x))) %>%
    dplyr::select(-splits)
  
  model_runs <- crossing(under_ratio_vec,
                         model_type_vec,
                         bag_vec) %>%
    # If there's no downsampling, there's no bagging
    mutate(bagging = ifelse(is.na(under_ratio),FALSE,TRUE)) %>%
    # Don't need to bag if not doing downsampling
    filter(!(!bagging & bag >1)) %>%
    # Want different seed each time downsampling is done, for bagging
    mutate(seed = sample.int(10^5,n()))

  pb <- progress_estimated(nrow(model_runs) * nrow(cv_splits))
  
  predictions <- model_runs %>%
    mutate(recipe = map2(seed,bagging,function(x,y){
      # Add downsampling to pre-processing recipe if bagging
      ifelse(!y,
             tmp_recipe <- fl_rec,
             tmp_recipe <- fl_rec %>%
               step_downsample("offender",under_ratio=y,seed=x,skip=TRUE))
      return(tmp_recipe)
      # Create workflow based on random forest hyperparameter and downsampling pre-processing recipe
    }),
    workflow = map2(recipe,model_type,function(x,y){
      # Add either random forest or svm to worflow, along with appropriate recipe
      workflow() %>%
               add_model(rf_mod) %>%
               add_recipe(x)})) %>%
    # Remove unnecessary columns
    dplyr::select(-seed,-recipe) %>%
    # For each workflow, do entire model fitting process across all folds
    mutate(predict = map(workflow,function(x){
      analysis_data %>%
        mutate(predict = map2(analysis,assessment,function(y,z){
          tmp <- fit(x,y) %>%
            # Predict assessment data using fit
            predict(z, type = "prob") %>%
            # Add predictions to assessment data
            bind_cols(z) %>%
            # Select relevant columns
            dplyr::select(mmsi,year,offender,.pred_1)
          # Increment progress bar
          pb$tick()$print()
          return(tmp)
        })) %>%
        dplyr::select(id,predict) %>%
        unnest(predict)
    })) %>%
    dplyr::select(-workflow) %>%
    unnest(predict)
  
  bags_to_show <- max(bag_vec)
  bag_performance <- map_df(bags_to_show,function(x){
    tmp_performance <- predictions %>%
      filter(bag<=x) %>%
      # For each fold and hyperparameter set, take average prediction for each vessel-year (mmsi and year) across bags
      group_by(id,model_type,under_ratio,mmsi,year) %>%
      summarize(offender = offender[1],
                .pred_1 = mean(.pred_1,na.rm=TRUE)) %>%
      ungroup()  %>%
      # Make offender =1 reference level
      mutate(offender = relevel(as.factor(offender),"1")) %>%
      # Then we will calculate performance metrics, by threshold, for each fold and hyperparameter set
      group_by(id,model_type,under_ratio) %>%
      nest() %>%
      ungroup() %>%
      # Calculated metrics will include recall, modified_f1, and detection_prevalence
      mutate(threshold_perf = map(data,~threshold_perf_custom(.x,
                                                              truth = offender,
                                                              estimate = .pred_1,
                                                              thresholds=seq(0.01,0.99,0.01)))) %>%
      dplyr::select(-data) %>%
      unnest(threshold_perf) %>%
    group_by(model_type,under_ratio,.threshold,.metric) %>%
          # Get mean, min, and SD of performance across folds
      # Don't compute if fold contains NAs or NaNs
    summarize(mean_performance = mean(.estimate),
              sd_performance = sd(.estimate),
              min_performance = min(.estimate)) %>%
    ungroup() %>%
    group_by(model_type,under_ratio) %>%
    nest() %>%
    ungroup() %>%
    # Now we will find the optimial threshold for maximizing the modified F1 estimator
    # Maximize the mean, while ensuring min performance across folds is at least 0
    mutate(optimal_threshold = map2_dbl(data,under_ratio,~.x %>%
                                          filter(min_performance >0 | is.na(.y)) %>%
                                         filter(.metric=="modified_f1") %>%
                                         arrange(desc(mean_performance)) %>%
                                         slice(1) %>%
                                         .$.threshold),
           # optimized_metrics will contain metrics with optimized modified F1 estimator
           optimized_metrics = map2(data,optimal_threshold,~.x %>%
                                      filter(.threshold==.y))) %>% 
    unnest(optimized_metrics) %>%
      dplyr::select(-optimal_threshold,-data) %>%
      mutate(number_bags = x)
    print(paste(x,"complete"))
    tmp_performance
  })
  
  optimized_model <- tibble(model_type = "random_forest",
                            bagging = TRUE,
                            under_ratio = 1,
                            number_bags = max(bag_vec))
  
  cv_performance <- bag_performance %>% 
    mutate(bagging = ifelse(is.na(under_ratio),FALSE,TRUE)) %>%
    inner_join(optimized_model,by = c("model_type", "number_bags", "under_ratio","bagging"))
  
  optimized_model$.threshold <- cv_performance$.threshold[1]
  
  set.seed(101)
  # Define all model runs
  model_runs_final <- optimized_model %>%
    crossing(tibble(bag = seq(optimized_model$number_bags))) %>%
    mutate(seed = sample.int(10^5,n()))
  
  pb <- progress_estimated(nrow(model_runs_final))
  
  predictions_final <- model_runs_final %>%
    mutate(recipe = map2(seed,bagging,function(x,y){
      # Add downsampling to pre-processing recipe if bagging
      ifelse(!y,
             tmp_recipe <- fl_rec,
             tmp_recipe <- fl_rec %>%
               step_downsample("offender",under_ratio=y,seed=x,skip=TRUE))
      return(tmp_recipe)
      # Create workflow based on random forest hyperparameter and downsampling pre-processing recipe
    }),
    workflow = map2(recipe,model_type,function(x,y){
      # Add either random forest or svm to worflow, along with appropriate recipe
      workflow() %>%
               add_model(rf_mod %>%
                           set_args(importance = "impurity_corrected")) %>%
               add_recipe(x)})) %>%
    # Remove unnecessary columns
    dplyr::select(-seed,-recipe) %>%
    # For each workflow, do entire model fitting process across all folds
    mutate(fit = map(workflow,~fit(.x,training_df)),
           predict = map(fit,function(x){
             tmp <- x %>%
               # Predict assessment data using fit
               predict(prediction_df, type = "prob") %>%
               # Add predictions to assessment data
               bind_cols(prediction_df)%>%
               # Select relevant columns
               dplyr::select(mmsi,year,offender,.pred_1)
             # Increment progress bar
             pb$tick()$print()
             return(tmp)
           })) %>%
    dplyr::select(-workflow)
  
  as.numeric.factor <- function(x) {as.numeric(levels(x))[x]}
  predictions_figures <- predictions_final %>%  
    dplyr::select(-fit) %>%
    unnest(predict) %>%
    # Switch factors to numeric for faster computation, then switch back
    mutate(offender = as.numeric.factor(offender),
           mmsi = as.numeric.factor(mmsi),
           year = as.numeric.factor(year)) %>%
    # Take average prediction for each vessel-year (mmsi and year) across bags
    group_by(mmsi,year) %>%
    summarize(offender = offender[1],
              .pred_1 = mean(.pred_1,na.rm=TRUE)) %>%
    ungroup() %>% 
    mutate(mmsi = as.factor(mmsi),
           year = as.factor(year)) %>%
    left_join(prediction_df %>%
                dplyr::select(-offender),
              by=c("mmsi","year")) %>%
    mutate(Country = countrycode(flag,"iso3c","country.name"))%>%
    # Use optimal threshold found above
    mutate(class = ifelse(.pred_1 >optimized_model$.threshold,1,0))%>% 
    mutate(Prediction = ifelse(class == 1,"Positive","Negative")) %>%
    mutate(Label = ifelse(offender == 0,"Unlabeled","Positive"))
  
  n_fun <- function(x){
    return(data.frame(y = max(x) + 0.05, label = paste0("n = ",prettyNum(length(x),big.mark=","))))
  }
  
  class_fig <- predictions_figures  %>%
    ggplot(aes(x=Prediction,y=.pred_1,fill=Label)) + 
    geom_boxplot(position = position_dodge(width = 0.75)) + 
    coord_flip() + 
    geom_hline(yintercept = optimized_model$.threshold,linetype=2) +
    labs(y = "Model risk score",
         x = "Model\nclassification",
         title = paste0("Year assumption: ",year_assumption))+
    stat_summary(fun.data = n_fun, geom = "label",position = position_dodge(width = 0.75),show.legend = FALSE) +
    ylim(c(0,1)) +
    disco::scale_fill_disco(palette = "vibrant", "Training data\nlabel",direction=1,alpha=1,
                            guide = guide_legend(reverse = TRUE) ) +
    theme_bw() +
    theme(axis.title.y = element_text(angle=0,vjust=0.5))

  return(list(cv_performance = cv_performance,
               predictions_figures = predictions_figures))
})) %>% 
  mutate(cv_performance = map(robustness_results,function(x){x$cv_performance}),
         predictions_figures = map(robustness_results,function(x){x$predictions_figures})) %>%
  dplyr::select(-robustness_results)

robustness_cv <- robustness_results_df %>%
  dplyr::select(-predictions_figures) %>%
  unnest(cv_performance)

write_csv(robustness_cv,here::here("interim_data/s9_robustness_cv.csv"))


robustness_predictions_figures <- robustness_results_df %>%
  dplyr::select(-cv_performance) %>%
  unnest(predictions_figures)

write_csv(robustness_predictions_figures %>%
            dplyr::select(year_assumption,
                          include_vessel_characteristics,
                          mmsi,
                          year,
                          crew_size,
                          offender,
                          .pred_1,
                          flag,
                          gear,
                          class,
                          fldb_vessel_id),here::here("interim_data/s10_robustness_predictions_figures_private.csv"))

write_csv(robustness_predictions_figures %>%
            # Anonymize MMSI for public version
            mutate(mmsi_anonymous = anonymize(mmsi),.seed=101)%>%
            dplyr::select(mmsi_anonymous,
                          year_assumption,
                          include_vessel_characteristics,
                          year,
                          crew_size,
                          offender,
                          flag,
                          gear,
                          class,
                          fldb_vessel_id),here::here("interim_data/s10_robustness_predictions_figures.csv"))
```


\pagebreak

# Use BigQuery

```{r eval = params$use_big_query}
# Bigquery project
ucsb_project <- "ucsb-gfw"
# Use GFW account, but only for this project
gfw_project <- "world-fishing-827"
options(scipen = 20)
```


## High risk fishing effort  

```{r eval = params$use_big_query}
# Upload risk predictions
# This uses the private version of the file that has actual MMSIs
predictions_figures_private <- read.csv(here::here("interim_data/s4_final_model_predictions_private.csv"),stringsAsFactors = FALSE) %>%
  as_tibble() %>%
  dplyr::select(mmsi,year,gear,offender,fldb_vessel_id,prediction=.pred_1,class,crew_size,flag,Country) %>%
  mutate(mmsi = as.integer(mmsi))

bq_table(project = ucsb_project,table = "risk_predictions",dataset = "human_rights") %>%
  bq_table_upload(values = predictions_figures_private,
                  fields = as_bq_fields(predictions_figures_private),
                  write_disposition = "WRITE_TRUNCATE")

# Upload risk predictions from robustness checks
# This uses the private version of the file that has actual MMSIs
robustness_predictions_figures_private <- read.csv(here::here("interim_data/s10_robustness_predictions_figures_private.csv"),stringsAsFactors = FALSE) %>%
  as_tibble() %>%
  dplyr::select(year_assumption,include_vessel_characteristics,mmsi,year,gear,offender,fldb_vessel_id,prediction=.pred_1,class,flag) %>%
  mutate(mmsi = as.integer(mmsi))

bq_table(project = ucsb_project,table = "risk_predictions_robust",dataset = "human_rights") %>%
  bq_table_upload(values = robustness_predictions_figures_private,
                  fields = as_bq_fields(robustness_predictions_figures_private),
                  write_disposition = "WRITE_TRUNCATE")

# Summarize gridded fishing data

sql<-"SELECT
  mmsi,
  EXTRACT(YEAR
  FROM
    timestamp) year,
  FLOOR(lat / 0.25) * 0.25 + 0.125 lat_bin,
  FLOOR(lon / 0.25) * 0.25 + 0.125 lon_bin,
  SUM((CASE
        WHEN nnet_score2 = 1 AND NOT (distance_from_shore_m < 1000 AND speed < 1) THEN hours
      ELSE
      0
    END
      )) fishing_hours
FROM
  `ucsb-gfw.human_rights.all_vessel_positions`
GROUP BY
  mmsi,
  lat_bin,
  lon_bin,
  year"

# Only run this once since it's so expensive
#bq_project_query(gfw_project,sql, destination_table = bq_table(project = ucsb_project,table = "binned_fishing_base",dataset = "human_rights"),use_legacy_sql = FALSE, allowLargeResults = TRUE,write_disposition = "WRITE_TRUNCATE")

# Summarize risk for gridded fishing data, using baseline model assumption

sql <- "#standardSQL
WITH
  fishing_info AS(
  SELECT
    *
  FROM
    `ucsb-gfw.human_rights.binned_fishing_base`),
  extra_offsetting_filter AS(
  SELECT
    CAST(ssvid AS INT64) mmsi,
    year,
    TRUE extra_offsetting
  FROM
    `ucsb-gfw.human_rights.extra_offsetting_filter` ),
  vessel_info AS(
  SELECT
    mmsi,
    year,
    gear,
    flag,
    engine_power_kw
  FROM
    `ucsb-gfw.human_rights.vessel_info_all`),
  risk AS(
  SELECT
    mmsi,
    year,
    class,
    offender
  FROM
    `ucsb-gfw.human_rights.risk_predictions`),
  joined AS(
  SELECT
    *
  FROM
    vessel_info
  LEFT JOIN
    fishing_info
  USING
    (mmsi,
      year)
  LEFT JOIN
    risk
  USING
    (mmsi,
      year)
  LEFT JOIN
    extra_offsetting_filter
  USING
    (mmsi,
      year)
  WHERE extra_offsetting IS NULL)
SELECT
  year,
  gear,
  flag,
  lat_bin,
  lon_bin,
  SUM(fishing_hours) fishing_hours,
  SUM(fishing_hours * offender) known_offender_fishing_hours,
  SUM(fishing_hours * class) at_risk_fishing_hours,
  SUM(fishing_hours * engine_power_kw) fishing_kW_hours,
  SUM(fishing_hours * offender * engine_power_kw) known_offender_fishing_kW_hours,
  SUM(fishing_hours * class * engine_power_kw) at_risk_fishing_kW_hours
FROM
  joined
GROUP BY
  year,
  gear,
  flag,
  lat_bin,
  lon_bin"

bq_project_query(gfw_project,sql, destination_table = bq_table(project = ucsb_project,table = "grouped_fishing_info_predicted",dataset = "human_rights"),use_legacy_sql = FALSE, allowLargeResults = TRUE,write_disposition = "WRITE_TRUNCATE")

# Generate larger resolution gridded risk data for figure 3

sql <- "#standardSQL
  WITH base AS(
  SELECT
    year,
    gear,
    FLOOR(lat_bin / 0.5) * 0.5 + 0.25 lat_bin,
    FLOOR(lon_bin / 0.5) * 0.5 + 0.25 lon_bin,
    fishing_kW_hours,
    at_risk_fishing_kW_hours,
    known_offender_fishing_kW_hours
  FROM
    `ucsb-gfw.human_rights.grouped_fishing_info_predicted`
  WHERE
    fishing_kW_hours > 0)
SELECT
  year,
  gear,
  lat_bin,
  lon_bin,
  SUM(fishing_kW_hours) fishing_kW_hours,
  SUM(at_risk_fishing_kW_hours) at_risk_fishing_kW_hours,
  SUM(known_offender_fishing_kW_hours) known_offender_fishing_kW_hours,
  SUM(known_offender_fishing_kW_hours) / SUM(fishing_kW_hours) fraction_known_offender_fishing_kW_hours,
  SUM(at_risk_fishing_kW_hours) / SUM(fishing_kW_hours) fraction_at_risk_fishing_kW_hours
FROM
  base
WHERE
  gear IN('drifting_longlines',
    'trawlers',
    'squid_jigger')
GROUP BY
  year,
  gear,
  lat_bin,
  lon_bin"

grouped_messages <- bq_project_query(gfw_project, sql) %>%
  bq_table_download(max_results = Inf)%>% 
  filter(lon_bin<180,
         lat_bin<90)

write_csv(grouped_messages %>%
            filter(year==2018) %>%
            dplyr::select(-known_offender_fishing_kW_hours,
                  -fraction_known_offender_fishing_kW_hours) ,path=here::here("interim_data/s5_figure_3_data.csv"))
```

```{r eval=params$use_big_query}
# Also summarize risk for gridded data, across all model assumptions
sql <- "#standardSQL
WITH
  fishing_info AS(
  SELECT
    *
  FROM
    `ucsb-gfw.human_rights.binned_fishing_base`),
  extra_offsetting_filter AS(
  SELECT
    CAST(ssvid AS INT64) mmsi,
    year,
    TRUE extra_offsetting
  FROM
    `ucsb-gfw.human_rights.extra_offsetting_filter` ),
  vessel_info AS(
  SELECT
    mmsi,
    year,
    gear,
    flag,
    engine_power_kw
  FROM
    `ucsb-gfw.human_rights.vessel_info_all`),
  risk AS(
  SELECT
    mmsi,
    year,
    include_vessel_characteristics,
    year_assumption,
    class,
    offender
  FROM
    `ucsb-gfw.human_rights.risk_predictions_robust`),
  joined AS(
  SELECT
    *
  FROM
    vessel_info
  LEFT JOIN
    fishing_info
  USING
    (mmsi,
      year)
  LEFT JOIN
    risk
  USING
    (mmsi,
      year)
  LEFT JOIN
    extra_offsetting_filter
  USING
    (mmsi,
      year)
  WHERE extra_offsetting IS NULL)
SELECT
  year,
  year_assumption,
  include_vessel_characteristics,
  gear,
  flag,
  lat_bin,
  lon_bin,
  SUM(fishing_hours) fishing_hours,
  SUM(fishing_hours * offender) known_offender_fishing_hours,
  SUM(fishing_hours * class) at_risk_fishing_hours,
  SUM(fishing_hours * engine_power_kw) fishing_kW_hours,
  SUM(fishing_hours * offender * engine_power_kw) known_offender_fishing_kW_hours,
  SUM(fishing_hours * class * engine_power_kw) at_risk_fishing_kW_hours
FROM
  joined
GROUP BY
  year,
  year_assumption,
  include_vessel_characteristics,
  gear,
  flag,
  lat_bin,
  lon_bin"

bq_project_query(gfw_project,sql, destination_table = bq_table(project = ucsb_project,table = "grouped_fishing_info_predicted_robust",dataset = "human_rights"),use_legacy_sql = FALSE, allowLargeResults = TRUE,write_disposition = "WRITE_TRUNCATE")
```


\pagebreak

## High risk port visits

```{r eval = params$use_big_query}
## Port mapping
sql<-"#standardSQL
WITH
  anchorage_ids AS(
  SELECT
    s2id anchorage_id,
    iso3 port_iso3
  FROM
    `world-fishing-827.gfw_research.named_anchorages` ),
  port_visits AS(
  SELECT
    CAST(ssvid AS INT64) mmsi,
    EXTRACT(YEAR
    FROM
      trip_start) year,
    trip_start_anchorage_id anchorage_id
  FROM
    `world-fishing-827.gfw_research.vessel_voyages_v20190722`
  UNION ALL (
    SELECT
      CAST(ssvid AS INT64) mmsi,
      EXTRACT(YEAR
      FROM
        trip_start) year,
      trip_end_anchorage_id anchorage_id
    FROM
      `world-fishing-827.gfw_research.vessel_voyages_v20190722`) )
SELECT
  *
FROM
  port_visits
LEFT JOIN
  anchorage_ids
USING
  (anchorage_id)"

bq_project_query(gfw_project,sql, destination_table = bq_table(project = ucsb_project,table = "port_visits",dataset = "human_rights"),use_legacy_sql = FALSE, allowLargeResults = TRUE,write_disposition = "WRITE_TRUNCATE")

sql <- "WITH
  port_base AS(
  SELECT
    *
  FROM
    `ucsb-gfw.human_rights.port_visits`
  WHERE
    year < 2019),
  risk_predictions AS(
  SELECT
    class,
    offender,
    gear,
    year,
    mmsi
  FROM
    `ucsb-gfw.human_rights.risk_predictions`),
  psma AS(
  SELECT
    iso3 port_iso3,
    EXTRACT(YEAR
    FROM
      date_poc) psma_year
  FROM
    `ucsb-gfw.human_rights.foc_poc_database` ),
  joined AS(
  SELECT
    *
  FROM
    port_base
  JOIN
    risk_predictions
  USING
    (mmsi,
      year)),
  final AS(
  SELECT
    year,
    gear,
    port_iso3,
    COUNT(*) total_visits,
    SUM(class) high_risk_visits,
    SUM(offender) known_offender_visits,
    SUM(class) / COUNT(*) fraction_high_risk_visits
  FROM
    joined
  GROUP BY
    year,
    gear,
    port_iso3)
SELECT
  *,
IF
  (psma_year IS NULL
    OR year < psma_year,
    FALSE,
    TRUE) psma_country
FROM
  final
LEFT JOIN
  psma
USING
  (port_iso3)"

bq_project_query(gfw_project,sql, destination_table = bq_table(project = ucsb_project,table = "port_visits_risk_grouped",dataset = "human_rights"),use_legacy_sql = FALSE, allowLargeResults = TRUE,write_disposition = "WRITE_TRUNCATE")

sql <- "SELECT *
  FROM
    `ucsb-gfw.human_rights.port_visits_risk_grouped`"

port_data <- bq_project_query(gfw_project, sql) %>%
  bq_table_download(max_results = Inf)

write_csv(port_data,path=here::here("interim_data/s6_figure_4_data.csv"))
```

## Time at sea statistic

This query generates the fraction of total time at sea by included vessels in the analysis. By running this query, we find that "These vessels represent 33% of the total time at sea spent by all fishing vessels operating in this time period tracked by Global Fishing Watch."

```{r eval = params$use_big_query}
sql<-"WITH
included_vessels AS(
SELECT
year,mmsi,TRUE included
FROM
`ucsb-gfw.human_rights.risk_predictions`
),
base AS(
SELECT
  CAST(ssvid AS INT64) mmsi,year,
  activity.active_hours active_hours
FROM
  `world-fishing-827.gfw_research.vi_ssvid_byyear_v20190430`
  WHERE on_fishing_list_best),
joined AS(
 SELECT
mmsi,
year,
active_hours,
IF(included,TRUE,FALSE) included
FROM
base
LEFT JOIN
included_vessels
USING(mmsi,year)),
summary AS(
SELECT
SUM(active_hours) active_hours,
included
FROM joined
GROUP BY
included)
SELECT
(SELECT active_hours FROm summary WHERE included)/((SELECT active_hours FROm summary WHERE NOT included)+(SELECT active_hours FROm summary WHERE included)) fraction_active_hours_included"

fraction_active_hours_included <- bq_project_query(gfw_project, sql) %>%
  bq_table_download(max_results = Inf)
```

## Known registry vessel characteristics

```{r eval = params$use_big_query}
# Summarize how many vessels have known vessel characteristics
sql <- "WITH
  risk_predictions AS(
  SELECT
    mmsi
  FROM
    `ucsb-gfw.human_rights.risk_predictions`
  GROUP BY
    mmsi),
  known_info AS(
  SELECT
    CAST(identity.ssvid AS INT64) mmsi,
    feature.length_m,
    feature.tonnage_gt,
    feature.engine_power_kw,
    feature.crew
  FROM
    `world-fishing-827.vessel_database.matched_vessels_one_record_per_ssvid_v20200801`),
  joined_table AS(
  SELECT
    mmsi,
  IF
    (length_m IS NULL,
      0,
      1) length_m,
  IF
    (tonnage_gt IS NULL,
      0,
      1) tonnage_gt,
  IF
    (engine_power_kw IS NULL,
      0,
      1) engine_power_kw,
  IF
    (crew IS NULL,
      0,
      1) crew
  FROM
    risk_predictions
  LEFT JOIN
    known_info
  USING
    (mmsi))
SELECT
  COUNT(*) total_vessel_years,
  SUM(length_m) known_length_m,
  SUM(tonnage_gt) known_tonnage_gt,
  SUM(engine_power_kw) known_engine_power_kw,
  SUM(crew) known_crew
FROM
  joined_table"

known_vessel_info <- bq_project_query(gfw_project, sql) %>%
  bq_table_download(max_results = Inf)

write_csv(known_vessel_info,here::here("interim_data/s11_known_vessel_info.csv"))

```


\pagebreak

# Figures

```{r eval = TRUE}
# Read in cached final predictions
predictions_figures <- read.csv(here::here("interim_data/s4_final_model_predictions.csv"),stringsAsFactors = FALSE) %>%
  as_tibble() %>% 
  mutate(Prediction = ifelse(class == 1,"Positive","Negative")) %>%
  mutate(Label = ifelse(offender == 0,"Unlabeled","Positive"))

# Pull in forced labor vessel database
fldb_data <-read_csv(here::here("raw_data/s3_suspected_forced_labor_database.csv"))

# Pull in spatial forced labor risk data
grouped_messages <- read.csv(here::here("interim_data/s5_figure_3_data.csv"),stringsAsFactors = FALSE)

# Pull in port data
port_data <- read_csv(here::here("interim_data/s6_figure_4_data.csv")) %>%
  filter(!is.na(port_iso3))

# For each port country, summarize if there were known offender visits and predicted high-risk visits
# This will determine if there were any visits by known offenders across the entire 2012-2018 timeframe
port_visit_summary <- port_data %>%
  group_by(port_iso3) %>%
  summarize(has_known_offender_visits = ifelse(sum(known_offender_visits,na.rm=TRUE)>0,TRUE,FALSE),
            no_known_offender_visits = !has_known_offender_visits) %>%
  ungroup()

# For remaining analysis, just look at 2018
port_data <- port_data %>%
  filter(year==2018) %>%
  left_join(port_visit_summary,by= c("port_iso3"))

# Read in cached cross-validation results from robustness checks
robustness_cv <- read.csv(here::here("interim_data/s9_robustness_cv.csv"),stringsAsFactors = FALSE) %>%
  mutate(year_assumption = as.character(year_assumption))

# Read in cached final results from robustness checks
robustness_predictions_figures <- read.csv(here::here("interim_data/s10_robustness_predictions_figures.csv"),stringsAsFactors = FALSE) %>%
  mutate(year_assumption = as.character(year_assumption)) %>%
  mutate(Country = countrycode(flag,"iso3c","country.name"))%>%
  # Use optimal threshold found above
  mutate(Prediction = ifelse(class == 1,"Positive","Negative")) %>%
  mutate(Label = ifelse(offender == 0,"Unlabeled","Positive"))

# Pull in summary of known vessel registry info
known_vessel_info <- read_csv(here::here("interim_data/s11_known_vessel_info.csv"))

# Pull in fldb classification data for figure s1
predictions_fldb_mmsi <- read_csv(here::here("interim_data/s12_figure_s1_data.csv"))
```

## Figure 1: Training data model feature summary

```{r eval = params$generate_figures}
training_df_figure <- filtered_main_df %>%
  mutate(ais_type = ifelse(ais_type == "A",1,0))%>%
  dplyr::select(-year,-source_id,-fldb_vessel_id,-flag)


asinh_trans <- function(){
  scales::trans_new(name = 'asinh', transform = function(x) asinh(x), 
            inverse = function(x) sinh(x))
}

indicator_figure_data <- recipes::recipe(offender ~ ., data = head(training_df_figure)) %>%
  recipes::update_role(mmsi, new_role = "id variable") %>%
  recipes::update_role(gear, new_role = "id variable") %>%
  # Box-Cox transformation on all numeric
  recipes::step_BoxCox(all_numeric())%>%
  step_normalize(all_predictors()) %>%
  prep(training_df_figure) %>%
  juice() %>%
  pivot_longer(cols=c(-"mmsi",-"gear",-"offender"),names_to = "indicator") %>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "Longliner",
                          gear=="squid_jigger" ~ "Squid jigger",
                          TRUE ~ "Trawler")) %>%
    left_join(feature_lookup,by="indicator") %>%
  mutate(offender=ifelse(offender==1,"Positive","Unlabeled"))  %>%
  mutate(indicator_type = case_when(indicator_type =="directly_observed" ~ "Directly\nobserved",
                                    indicator_type =="gfw_fishing_model" ~ "GFW\nfishing\nalgorithm",
                                    indicator_type =="registry_or_gfw_vessel_model" ~ "Vessel\nregistry\nor GFW\nvessel\nalgorithm"))

indicator_figure <- indicator_figure_data %>%
  ggplot(aes(x = reorder(indicator_name,desc(indicator_name)),y=value,color=factor(offender))) +
  geom_boxplot(position=position_dodge(width=1),outlier.size = 0.1) +
  disco::scale_color_disco(palette = "vibrant", "Training data label",direction=1,alpha=1,
                           guide = guide_legend(reverse = TRUE) ) +
  coord_flip() +
  facet_grid_paginate(indicator_type~gear,drop=TRUE,space="free",scales="free_y") +
  #facet_col("indicator_type",scales="free_y",drop=TRUE,space="free") +
  #facet_grid(indicator_type~gear,scales="free_y") +
  scale_y_continuous(trans="asinh",breaks = scales::pretty_breaks(2)) +
    labs(
      x = "",
      y = "Box-Cox transformed, centered, and scaled\n vales (inverse hyperbolic sine scale)"
    ) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1,size=8),
        legend.position = "bottom",
        legend.direction = "horizontal",
        strip.text.y.right = element_text(angle = 0),
        strip.background =element_rect(fill=NA))

indicator_figure

ggsave(here::here("output_figures/figure_1.pdf"),indicator_figure,width=8,height=8,device="pdf",dpi=1200)
```

\pagebreak

## Figure 2 - Forced labor risk by fishing fleet

### Figure 2 using point estimates (not shown in paper)

```{r eval = TRUE}
flag_predictions_other <- predictions_figures %>%
  filter(class == 1) %>%
  group_by(Country,gear) %>%
  summarize(count_vessels = n_distinct(mmsi_anonymous)) %>%
  ungroup() %>%
  group_by(gear) %>%
  mutate(fraction = count_vessels / sum(count_vessels)) %>%
  mutate(Other = ifelse(fraction<0.025,TRUE,FALSE))
```

```{r eval = params$generate_figures}
vessels_fig_total <- predictions_figures  %>%
  filter(class == 1) %>%
  left_join(flag_predictions_other,by=c("Country","gear")) %>%
  mutate(Country = ifelse(Other,"Other",Country))%>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "Longliner",
                          gear=="squid_jigger" ~ "Squid jigger",
                          TRUE ~ "Trawler")) %>%
  group_by(Country,gear,year) %>%
  summarize(`Number of vessels` = n()) %>%
  ungroup() %>%
  ggplot(aes(x = year, y = `Number of vessels`, fill = Country)) +
  geom_bar(stat = "identity", color = "black",size=0.25) +
  #coord_flip() +
  labs(x="",title="(A) Number of high-risk vessels",y="") +
  theme_bw() +
  disco::scale_fill_disco(palette = "rainbow", "Flag") +
  facet_grid(.~gear)+
  scale_y_continuous(labels=scales::comma)+ 
  scale_x_continuous(breaks = unique(predictions_figures$year)) +
  theme(strip.background =element_rect(fill=NA),
        axis.text.x = element_blank(),
        panel.grid.minor = element_blank())

vessels_fig_percentage <- predictions_figures %>%
  group_by(year,gear) %>%
  summarize(fraction_high_risk = sum(class) / n())%>%
  ungroup() %>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "Longliner",
                          gear=="squid_jigger" ~ "Squid jigger",
                          TRUE ~ "Trawler")) %>%
  ggplot(aes(x = year, y = fraction_high_risk)) +
  geom_bar(stat="identity",color="black") +
  facet_grid(.~gear) +
  theme_bw()+
  scale_y_continuous(labels=scales::percent,limits = c(0,1))+ 
  scale_x_continuous(breaks = unique(predictions_figures$year),
                   labels = unique(predictions_figures$year)) +
  theme(strip.background =element_rect(fill=NA),
        axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.minor = element_blank())+
  labs(x="",title="(B) Percentage of total vessels that are high-risk",y="")

legend <- get_legend(
  # create some space to the left of the legend
  vessels_fig_total
)

vessels_fig <- cowplot::plot_grid(plot_grid(vessels_fig_total+ theme(legend.position="none"),
                             vessels_fig_percentage,
                             align = "v", 
                             ncol = 1),
                   legend,
                   ncol=2,
                   rel_widths = c(4,1))  
vessels_fig
```

\pagebreak

### Figure 2 using range estimates from robustness checks

```{r eval = params$generate_figures}
vessels_fig_data_summary_other <- robustness_predictions_figures %>%
  filter(include_vessel_characteristics) %>%
  filter(class == 1) %>%
  group_by(Country,gear) %>%
  summarize(count_vessels_years = n())%>%
  group_by(gear) %>%
  mutate(fraction = count_vessels_years / sum(count_vessels_years)) %>%
  ungroup() %>%
  mutate(Other = ifelse(fraction<0.025,TRUE,FALSE))  %>%
  dplyr::select(Country,gear,Other)

vessels_fig_data_summary_combined <- robustness_predictions_figures %>%
  filter(include_vessel_characteristics) %>%
  filter(class == 1) %>%
  left_join(vessels_fig_data_summary_other,by=c("Country","gear")) %>%
  mutate(Country = ifelse(Other,"Other",Country)) %>%
  group_by(Country,gear,year,year_assumption,include_vessel_characteristics) %>%
  summarize(count_vessels = n_distinct(mmsi_anonymous)) %>%
  ungroup() %>%
  group_by(Country,gear,year) %>%
  summarize(count_vessels_average = mean(count_vessels,na.rm=TRUE),
            count_vessels_min = min(count_vessels,na.rm=TRUE),
            count_vessels_max = max(count_vessels,na.rm=TRUE)) %>%
  ungroup() %>%
  group_by(gear) %>%
  complete(Country,year,fill = list(count_vessels_min=0,
                                                  count_vessels_max=0,
                                                  count_vessels_average=0)) %>%
  ungroup()

vessels_fig_total <- vessels_fig_data_summary_combined %>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "Longliner",
                          gear=="squid_jigger" ~ "Squid jigger",
                          TRUE ~ "Trawler")) %>%
  ggplot(aes(year)) + 
  geom_ribbon(aes(ymin=count_vessels_min, ymax=count_vessels_max,fill=Country,color=Country),alpha=0.5,size=0.25)+
  geom_line(aes(y=count_vessels_average,color=Country)) +
  labs(x="",title="(A) Number of high-risk vessels",y="") +
  theme_bw() +
  disco::scale_fill_disco(palette = "rainbow", "Flag") +
  disco::scale_color_disco(palette = "rainbow", "Flag") +
  facet_grid(.~gear)+
  scale_y_continuous(labels=scales::comma)+ 
 # scale_x_continuous(breaks = unique(robustness_predictions_figures$year)) +
  theme(strip.background =element_rect(fill=NA),
        axis.text.x = element_blank(),
        panel.grid.minor = element_blank())

vessels_fig_percentage <- robustness_predictions_figures %>%
  group_by(gear,year,year_assumption,include_vessel_characteristics) %>%
  summarize(fraction_high_risk = sum(class) / n())%>%
  ungroup() %>%
  group_by(year,gear) %>%
  summarize(fraction_high_risk_average = mean(fraction_high_risk,na.rm=TRUE),
            fraction_high_risk_min = min(fraction_high_risk,na.rm=TRUE),
            fraction_high_risk_max = max(fraction_high_risk,na.rm=TRUE))%>%
  ungroup() %>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "Longliner",
                          gear=="squid_jigger" ~ "Squid jigger",
                          TRUE ~ "Trawler")) %>%
  ggplot(aes(year)) +
  geom_ribbon(aes(ymin=fraction_high_risk_min, ymax=fraction_high_risk_max),alpha=0.5,size=0.25,color="black") +
  geom_line(aes(y=fraction_high_risk_average)) +
  facet_grid(.~gear) +
  theme_bw()+
  scale_y_continuous(labels=scales::percent,limits = c(0,1))+ 
  scale_x_continuous(breaks = unique(robustness_predictions_figures$year),
                     labels = unique(robustness_predictions_figures$year)) +
  theme(strip.background =element_rect(fill=NA),
        axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.minor = element_blank())+
  labs(x="",title="(B) Percentage of total vessels that are high-risk",y="")

legend <- get_legend(
  # create some space to the left of the legend
  vessels_fig_total
)

vessels_fig <- cowplot::plot_grid(plot_grid(vessels_fig_total+ theme(legend.position="none"),
                                            vessels_fig_percentage,
                                            align = "v", 
                                            ncol = 1),
                                  legend,
                                  ncol=2,
                                  rel_widths = c(4,1))  
vessels_fig

ggsave(here::here("output_figures/figure_2.pdf"),vessels_fig,width=7.5,height=5,device="pdf",dpi=1200)

```


\pagebreak

## Figure 3 - Spatial forced labor risk

```{r eval = params$generate_figures}
#map_projection <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
map_projection <- "+proj=eqearth +datum=WGS84 +wktext"
#map_projection <- "+proj=moll"
world_land <- rworldmap::getMap(resolution = "low")

world_land_sf <- world_land%>%
  st_as_sf() %>%
  st_transform(map_projection)

# Back bounding box of world, to use as black background
# vectors of latitudes and longitudes that go once around the 
# globe in 1-degree steps
lats <- c(90:-90, -90:90, 90)
longs <- c(rep(c(180, -180), each = 181), 180)

world_bbox_sf <- 
  list(cbind(longs, lats)) %>%
  st_polygon() %>%
  st_sfc( # create sf geometry list column
    crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
  ) %>% 
  st_sf() %>%
  st_transform(map_projection)

# Define unique spatial IDs for all grids
location_ids <- grouped_messages  %>%
  distinct(lon_bin,lat_bin) %>% 
  mutate(location_id = row_number()) 

# Convert grids to raster, then to polygons, then to sf, then re-project them
location_ids_spatial <- location_ids %>%
  raster::rasterFromXYZ(crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs") %>% 
  raster::rasterToPolygons() %>% 
  st_as_sf() %>%
  st_transform(crs = map_projection)

# Add spatial sf info to grids
grouped_messages <- grouped_messages %>%
              left_join(location_ids,by=c("lon_bin","lat_bin"))

grouped_messages_sf <- location_ids_spatial %>%
  left_join(grouped_messages,by="location_id")

map_palette <- "inferno"
map_palette_begin <- 0.2
na_color <- viridis::viridis(n=5,option=map_palette,begin=map_palette_begin)[1]

pal <- c(viridis::inferno(n = 8))[c(3,5:8)]
pal <- c("midnightblue",viridis::inferno(n = 7)[c(3:7)])

map_figure_rel <-   grouped_messages_sf %>%
  filter(!is.na(gear))%>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "(A) Longliner",
                          gear=="squid_jigger" ~ "(B) Squid jigger",
                          TRUE ~ "(C) Trawler")) %>%
  ggplot() +
  geom_sf(data = world_bbox_sf, color="grey30", fill="black") +
  geom_sf(aes(fill = fraction_at_risk_fishing_kW_hours,color=fraction_at_risk_fishing_kW_hours),lwd=0) +
  geom_sf(data = world_land_sf, fill = "grey45", color = "grey45")+
  scale_fill_gradientn(name = "Percentage\nfishing\nhigh-risk", na.value = pal[1], values = c(0,0.1,.25,0.5,0.75,1),labels=paste0(seq(0,1,.25)*100,"%"),breaks=seq(0,1,.25),colors=pal)+
  scale_color_gradientn(name = "Percentage\nfishing\nhigh-risk", na.value = pal[1], values = c(0,0.1,.25,0.5,0.75,1),labels=paste0(seq(0,1,.25)*100,"%"),breaks=seq(0,1,.25),colors=pal)  +
  theme_minimal() +
  theme(panel.background = element_blank(),
        panel.grid.minor = element_line(colour = "black"),
        panel.grid.major = element_line(colour = "black"),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        strip.background = element_rect(fill=NA,color=NA),
        text = element_text(size=15),
        legend.direction = "vertical"
  )  +
  labs(
    x = "",
    y = "")  +
  facet_wrap(.~gear,ncol=1)

map_figure_rel

ggsave(here::here("output_figures/figure_3.pdf"),map_figure_rel,width=6,height=7.5,device="pdf",dpi=1200)
```

\pagebreak

## Figure 4: Port visits by high-risk vessels

```{r eval = params$generate_figures}
world_land_ports <- world_land %>% 
  st_as_sf() %>%
  mutate(port_iso3 = as.character(ISO3.1)) %>%
  dplyr::select(geometry,port_iso3) %>%
  left_join(port_data %>%
               filter(year==2018),by="port_iso3") %>%
  st_transform(map_projection)

port_map_figure_rel <-   world_land_ports   %>%
  filter(!is.na(gear))%>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "(A) Longliner",
                          gear=="squid_jigger" ~ "(B) Squid jigger",
                          TRUE ~ "(C) Trawler"))%>%
  ggplot()  +
  geom_sf(data = world_bbox_sf, color="grey30", fill="black")+
  geom_sf(data = world_land_sf, fill = "grey45", color = "grey45") +
  geom_sf(aes(fill = fraction_high_risk_visits,
                   color = has_known_offender_visits),
          size=0.1) +
  scale_fill_gradientn(name = "Percentage\nport visits\nhigh-risk", na.value = pal[1], values = c(0,0.1,.25,0.5,0.75,1),labels=paste0(seq(0,1,.25)*100,"%"),breaks=seq(0,1,.25),colors=pal)  +
  scale_color_manual("Has known\npositive\nport visits",
                     values = c("grey40","white")) +
  theme_minimal() +
  theme(panel.background = element_blank(),
        panel.grid.major = element_line(colour = "black"),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        text = element_text(size=15),
        legend.direction = "vertical",
        legend.key = element_rect(fill = "black")
  ) +
  labs(
    x = "",
    y = ""
  ) + 
  facet_wrap(.~gear,ncol=1)+ 
  guides(color = guide_legend(override.aes = list(size = 1,
                                                  fill = "black")))

port_map_figure_rel

ggsave(here::here("/output_figures/figure_4.pdf"),port_map_figure_rel,width=6,height=7.5,device="pdf",dpi=1200)      
```

\pagebreak

## Figure S1 - Positive vessel cases

```{r eval = params$generate_figures}
# Make figure summarizing ILO indicators
# Summarize the number and fraction of forced labor vessels in operation during each number
still_in_operation_stats <- predictions_fldb_mmsi %>%
  group_by(year) %>%
  summarize(count_in_operation = n(),
            fraction_in_operation = n()/n_distinct(predictions_fldb_mmsi$fldb_vessel_id),
            count_high_risk = sum(class),
            fraction_high_risk = sum(class)/count_in_operation)

operation_data <- predictions_fldb_mmsi%>%
  mutate(offender=as.factor(offender),
         class=as.factor(class),
         name = as.factor(year))%>%
  mutate(fldb_vessel_id = as.character(fldb_vessel_id))%>%
  mutate(indicator_type = "operation_status")

ilo_data <- fldb_data %>%
  distinct(fldb_vessel_id,forced_labor_indicator)%>%
  separate_rows(forced_labor_indicator,sep = ' \\| ') %>%
  mutate(forced_labor_indicator = case_when(forced_labor_indicator == "Abusive working and living conditons" ~ "Abusive working and living conditions",
                                            forced_labor_indicator == "Witholding of wages" ~ "Withholding of wages",
                                            TRUE ~ forced_labor_indicator)) %>%
  mutate(fldb_vessel_id = as.character(fldb_vessel_id)) %>%
  rename(name = forced_labor_indicator)%>%
  mutate(indicator_type = "ilo_indicator") %>%
  mutate(offender=as.factor(1),
         class=as.factor(1))

case_data <- fldb_data %>%
  distinct(fldb_vessel_id,eyewitness_account,official_investigation,non_official_investigations,arrests_made,charges_filed,conviction_made,penalties_sanctioned) %>%
  rename(`Penalties sanctioned` = penalties_sanctioned,
         `Eyewitness account` = eyewitness_account,
         `Official investigation` = official_investigation,
         `Non-official investigation` = non_official_investigations,
         `Arrests made` = arrests_made,
         `Charges filed` = charges_filed,
         `Conviction made` = conviction_made) %>%
  pivot_longer(cols = -fldb_vessel_id) %>%
  filter(!is.na(value))  %>%
  mutate(fldb_vessel_id = as.character(fldb_vessel_id)) %>%
  mutate(indicator_type = "case_info") %>%
  dplyr::select(-value) %>%
  mutate(offender=as.factor(1),
         class=as.factor(1))

combined_case_data <- bind_rows(ilo_data,case_data,operation_data)

# Summarize data by indicator, for organizing figure
ilo_summary_data <- ilo_data %>%
  group_by(name) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  arrange(count)
# Summarize data by vessel, for organizing figure
ilo_summary_vessel<- ilo_data %>%
  group_by(fldb_vessel_id) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  arrange(-count)

# Summarize data by case info, for organizing figure
case_summary_data <- rev(c("Eyewitness account",
                           "Non-official investigation",
                       "Official investigation",
                       "Arrests made",
                       "Charges filed",
                       "Conviction made",
                       "Penalties sanctioned"))

# Define factors for figure
summary_data <- c(ilo_summary_data$name,case_summary_data,rev(seq(2012,2018)))

ilo_indicator_fig <- combined_case_data %>%
  mutate(class = ifelse(class=="1","Positive","Negative") %>% fct_relevel("Positive")) %>%
  mutate(offender = ifelse(offender=="1","Positive","Not included")%>% fct_relevel("Positive")) %>%
  mutate(name = fct_relevel(name,summary_data))%>%
  mutate(fldb_vessel_id = fct_relevel(fldb_vessel_id,ilo_summary_vessel$fldb_vessel_id)) %>%
  mutate(indicator_type = case_when(indicator_type=="case_info"~"(B) Case status information",
                                    indicator_type=="ilo_indicator" ~ "(A) ILO forced labor indicator",
                                    TRUE ~ "(C) Vessel operation status and model classification")) %>%
  mutate(indicator_type = fct_relevel(indicator_type,"(A) ILO forced labor indicator")) %>%
  ggplot(aes(x=fldb_vessel_id,y=name,fill=interaction(class,indicator_type),color=interaction(offender,indicator_type))) +
  facet_col("indicator_type",scales="free_y",drop=TRUE,space="free") +
  geom_point(shape = 21,stroke=1.5,size=4) +
  theme_bw() +
  theme(axis.text.x = element_blank(),
        legend.position = "bottom",
        strip.background = element_blank(),
        strip.text = element_text(hjust=0,angle = 0)) +
  labs(x = "Reported vessel case",y="") +
  scale_color_manual("(C) Training\ndata label",values = c("black","black","red","black"),
                     breaks = c("Positive.(C) Vessel operation status and model classification","Not included.(C) Vessel operation status and model classification"),
                     labels = c("Positive","Not included")) +
  scale_fill_manual("(C) Model\nclassification",values = c("black","black","gray30","gray80"),
                     breaks = c("Positive.(C) Vessel operation status and model classification","Negative.(C) Vessel operation status and model classification"),
                     labels = c("Positive","Negative")) +
  guides(size = FALSE,
         fill = guide_legend(override.aes = list(size=4),nrow=2),
         color = guide_legend(override.aes = list(size=4),nrow=2,order = 1))

ilo_indicator_fig

ggsave(here::here("output_figures/figure_s1.pdf"),ilo_indicator_fig,width=7.5,height=7.5,device="pdf",dpi=1200)
```

\pagebreak

## Figure S2 - Summary of training data labels

```{r eval = params$generate_figures}

training_dataset_fig <- sort(unique(main_df$years_until_caught)) %>%
  map_df(function(x){
    main_df %>%
  # Only only include vessel-years from positives in the year prior to being caught
  filter((offender == 1 & years_until_caught <= x) | offender == 0) %>%
      group_by(year,gear,offender) %>%
      summarize(count=n()) %>%
      ungroup() %>%
      mutate(year_assumption = x)
  }) %>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "Longliner",
                          gear=="squid_jigger" ~ "Squid jigger",
                          TRUE ~ "Trawler"))%>%
  mutate(offender = ifelse(offender==0,"Unlabeled","Positive")) %>%
  ggplot(aes(x=year,y=count,fill=gear)) + 
  geom_bar(stat="identity",color="black",size=0.25) + 
  #coord_flip() + 
  facet_grid(offender~year_assumption,scales="free_y")+
  disco::scale_fill_disco(palette = "bright", "Gear") +
  theme_bw() +
  labs(x="",y="")+
  scale_x_continuous(breaks = seq(2012,2018),labels=c(2012,"","",2015,"","",2018)) +
  scale_y_continuous(labels=scales::comma_format(accuracy=1))+ 
  theme(strip.background =element_rect(fill=NA,
                                       color = NA),
        strip.text.y = element_text(hjust=0,angle=0),
        legend.position = "bottom",
        legend.direction = "horizontal",
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(angle = 45,hjust=1),
        axis.title.y = element_text(angle=0,vjust=0.5))

training_dataset_fig
ggsave(here::here("output_figures/figure_s2.pdf"),training_dataset_fig,width=7.5,height=4.5,device="pdf",dpi=1200)
```

\pagebreak

## Figure S3 - Cross-validation performance

```{r eval = params$generate_figures}
# Which bags to show in the figure
bags_to_show <- c(1,10,50,100)
cv_performance_fig <- bag_performance %>%
  filter(!is.na(under_ratio) | number_bags ==1) %>%
  mutate(number_bags = paste0("Bags: ",number_bags) %>%
           fct_relevel(paste0("Bags: ",bags_to_show))) %>%
mutate(under_ratio = ifelse(is.na(under_ratio),"N",under_ratio) %>%
           fct_relevel("N")) %>%
  mutate(model_type=ifelse(model_type=="random-forest","Random forest","Support vector\nmachine")) %>%
  mutate(error_min = pmax(mean_performance-sd_performance,0),
         error_max = ifelse(.metric=="recall",pmin(mean_performance+sd_performance,1),mean_performance+sd_performance)) %>%
#  filter(.metric %in% c("recall","modified_f1")) %>% 
    mutate(.metric = case_when(.metric=="recall"~"Recall",
                               .metric=="modified_f1"~"Modified F1",
                               TRUE~"Detection\nprevalence")) %>%
  mutate(.metric = fct_relevel(.metric,"Recall","Detection\nprevalence")) %>%
  ggplot(aes(x = factor(under_ratio),y=mean_performance,color=model_type)) + 
  geom_point(position=position_dodge(width=0.75)) +
  geom_errorbar(aes(ymin=error_min, ymax=error_max),position=position_dodge(width=0.75),width=0.5) + 
  facet_grid(.metric~number_bags,scales="free",switch="y") +
  labs(x = "Downsampling ratio",
       y = "Mean\nacross\nfolds")+ 
  theme_bw() +
  theme(strip.background =element_rect(fill=NA),
        axis.title.y = element_text(angle=0,vjust=0.5),
        strip.text.y.left = element_text(angle=0,vjust=0.5)) +
  disco::scale_color_disco("Model type")

cv_performance_fig

ggsave(here::here("output_figures/figure_s3.pdf"),cv_performance_fig,width=7.5,height=5,device="pdf",dpi=1200)
```

\pagebreak

## Figure S4: Classification by training label

```{r  eval = params$generate_figures}
n_fun <- function(x){
  return(data.frame(y = max(x) + 0.05, label = paste0("n = ",prettyNum(length(x),big.mark=","))))
}

class_fig <- predictions_figures  %>%
  ggplot(aes(x=Prediction,y=.pred_1,fill=Label)) + 
  geom_boxplot(position = position_dodge(width = 0.75)) + 
  coord_flip() + 
  geom_hline(yintercept = optimized_model$.threshold,linetype=2) +
  labs(y = "Model risk score",
       x = "Model\nclassification")+
    stat_summary(fun.data = n_fun, geom = "label",position = position_dodge(width = 0.75),show.legend = FALSE) +
  ylim(c(0,1)) +
  disco::scale_fill_disco(palette = "vibrant", "Training data\nlabel",direction=1,alpha=1,
                           guide = guide_legend(reverse = TRUE) ) +
  theme_bw() +
  theme(axis.title.y = element_text(angle=0,vjust=0.5))

class_fig

ggsave(here::here("output_figures/figure_s4.pdf"),class_fig,width=7.5,height=3.5,device="pdf",dpi=1200)
```

\pagebreak

## Figure S5: Variable importance

```{r eval = params$generate_figures}
importance_fig <- read_csv(here::here("interim_data/s8_figure_s5_data.csv")) %>%
  ggplot(aes(x = reorder(indicator_name,Importance), y = Importance)) +
  geom_bar(stat = "identity",color="black") +
  coord_flip() +
  labs(x = "",
       y = "Mean importance across bags") +
  theme_bw() +
  theme(panel.grid.minor = element_blank())

importance_fig

ggsave(here::here("output_figures/figure_s5.pdf"),importance_fig,width=7.5,height=5,device="pdf",dpi=1200)
```

\pagebreak

## Figure S6 - Cross-validation results from robustness checks

```{r eval = params$generate_figures}
robustness_cv_fig <- robustness_cv %>%
  mutate(error_min = pmax(mean_performance-sd_performance,0),
         error_max = ifelse(.metric=="recall",
                            pmin(mean_performance+sd_performance,1),
                            mean_performance+sd_performance))%>%
  mutate(.metric = case_when(.metric=="recall"~"Recall",
                             .metric=="modified_f1"~"Modified F1",
                             TRUE~"Detection\nprevalence")) %>%
  mutate(include_vessel_characteristics = ifelse(include_vessel_characteristics,"Yes","No") %>%
           fct_relevel("Yes")) %>%
  ggplot(aes(x = year_assumption,y=mean_performance,color=include_vessel_characteristics)) +
  geom_point(position=position_dodge(width=0.75)) +
  geom_errorbar(aes(ymin=error_min, ymax=error_max),position=position_dodge(width=0.75),width=0.5) + 
  facet_grid(.metric~.,scales="free_y",switch="y") +
  labs(x = "Positive training data set label year assumption",
       y = "Mean\nacross\nfolds")+ 
  theme_bw() +
  theme(strip.background =element_rect(fill=NA),
        axis.title.y = element_text(angle=0,vjust=0.5),
        strip.text.y.left = element_text(angle=0,vjust=0.5)) +
  disco::scale_color_disco("Include\nvessel\ncharacteristics",palette="muted") +
  ylim(c(0,NA))

robustness_cv_fig

ggsave(here::here("output_figures/figure_s6.pdf"),robustness_cv_fig,width=7.5,height=5,device="pdf",dpi=1200)

```

\pagebreak

## Figure S7 - Final model results from robustness checks

```{r eval = TRUE}
robustness_final <- robustness_predictions_figures %>%
  group_by(year_assumption,include_vessel_characteristics) %>%
  nest() %>%
  mutate(results = map(data,function(x){

    tmp_pred <- x
    
    n_predicted_positive_vessel_years <- tmp_pred %>%
      filter(Prediction == "Positive") %>%
      nrow()

    n_predicted_positive_vessel_years_new <- tmp_pred %>%
      filter(Prediction == "Positive" & Label == "Unlabeled") %>%
      nrow()
    
    fraction_positive_vessel_years <- n_predicted_positive_vessel_years / nrow(tmp_pred)
    
    
    n_predicted_positive_vessels <- tmp_pred %>%
      filter(Prediction == "Positive") %>%
      distinct(mmsi_anonymous) %>%
      nrow()
    
    fraction_positive_vessels <- n_predicted_positive_vessels / length(unique(tmp_pred$mmsi_anonymous))
    
    n_positive_crew <- tmp_pred %>%
      filter(Prediction == "Positive") %>%
      distinct(mmsi_anonymous,crew_size) %>%
      .$crew_size %>%
      sum()
    
    correct_positives <- tmp_pred %>%
      filter(Prediction == "Positive" & Label == "Positive") %>%
      nrow()
    
    fraction_correct_positives <- correct_positives / (tmp_pred %>%
                                                         filter( Label == "Positive") %>%
                                                         nrow())
    
    riskiest_gears <- tmp_pred %>%
      group_by(gear,class) %>%
      summarize(count = n()) %>%
      ungroup() %>%
      pivot_wider(names_from = "class",values_from="count") %>%
      mutate(fraction_risky = `1` / (`1` + `0`)) %>%
      rename(low_risk = `0`,
             high_risk = `1`)
    
    riskiest_fleets <-tmp_pred  %>%
      filter(class == 1) %>%
      group_by(mmsi_anonymous,Country,gear,year) %>%
      summarize(crew_size = mean(crew_size)) %>%
      ungroup() %>%
      group_by(Country,gear) %>%
      summarize(`Number of vessels` = n(),
                `Number of crew` = sum(crew_size)) %>%
      ungroup() %>%
      gather("indicator","count",-gear,-Country) %>%
      mutate(indicator = fct_relevel(indicator,c("Number of vessels","Number of crew"))) %>%
      filter(indicator == "Number of vessels") %>%
      arrange(desc(count)) %>%
      slice(1:5)

    return(tibble(n_predicted_positive_vessel_years = n_predicted_positive_vessel_years,
                  n_predicted_positive_vessels = n_predicted_positive_vessels,
                  n_predicted_positive_vessel_years_new = n_predicted_positive_vessel_years_new,
                  fraction_positive_vessels = fraction_positive_vessels,
                  fraction_positive_vessel_years = fraction_positive_vessel_years,
                  n_positive_crew = n_positive_crew,
                  fraction_correct_positives = fraction_correct_positives,
                  riskiest_gears = list(riskiest_gears),
                  riskiest_fleets = list(riskiest_fleets)))
  })) %>%
  ungroup() %>%
  dplyr::select(-data)

robustness_gears <- robustness_final %>%
  unnest(results) %>%
  dplyr::select(year_assumption,include_vessel_characteristics,riskiest_gears) %>%
  unnest(riskiest_gears)

robustness_fleets <- robustness_final %>%
  unnest(results) %>%
  dplyr::select(year_assumption,include_vessel_characteristics,riskiest_fleets) %>%
  unnest(riskiest_fleets)

robustness_final <- robustness_final  %>%
  unnest(results) %>%
  dplyr::select(-riskiest_gears,-riskiest_fleets)
```

```{r eval = params$generate_figures}
robustness_final_fig <- robustness_final%>%
  dplyr::select(-n_predicted_positive_vessel_years_new) %>%
  rename(`Fraction of correctly\nidentified true positives` = fraction_correct_positives,
         `Fraction of vessels\nidentified as positives` = fraction_positive_vessels,
         `Fraction of vessel-years\nidentified as positives` = fraction_positive_vessel_years,
         `Number of vessels\nidentified as positives` = n_predicted_positive_vessels,
         `Number of vessel-years\nidentified as positives` = n_predicted_positive_vessel_years,
         `Number of crew\nworking on\npositive vessels` = n_positive_crew) %>%
  pivot_longer(-c(year_assumption,include_vessel_characteristics)) %>%
  mutate(include_vessel_characteristics = ifelse(include_vessel_characteristics,"Yes","No") %>%
           fct_relevel("Yes")) %>%
  ggplot(aes(x = year_assumption,y=value,fill=include_vessel_characteristics)) +
  geom_bar(stat = "identity",position=position_dodge(width=1),color="black") +
  facet_grid(name~.,scales="free_y",switch="y") +
  labs(x = "Positive training data set label year assumption",
       y = "")+ 
  theme_bw() +
  theme(strip.background =element_rect(fill=NA),
        axis.title.y = element_text(angle=0,vjust=0.5),
        strip.text.y.left = element_text(angle=0,vjust=0.5)) +
  disco::scale_fill_disco("Include\nvessel\ncharacteristics",palette="muted") +
  scale_y_continuous(labels=scales::comma,limits = c(0,NA))

robustness_final_fig

ggsave(here::here("output_figures/figure_s7.pdf"),robustness_final_fig,width=7.5,height=7.5,device="pdf",dpi=1200)

```

\pagebreak

# Statistics

We select our optimized model variation to be random forest with 100 bags and a downsampling ratio of 1. This specification has the following mean performance across the 10 folds: modified F1 score: `r cv_performance %>% filter(.metric=="modified_f1") %>% .$mean_performance %>% signif(2)`; recall: `r cv_performance %>% filter(.metric=="recall") %>% .$mean_performance %>% signif(2)`; and detection prevalence: `r cv_performance %>% filter(.metric=="detection_prevalence") %>% .$mean_performance %>% signif(2)`.

## Using base model assumptions

```{r eval = TRUE}
# What is the number of unique high-risk vessels (vessels that were high-risk during at least 1 year)?
number_high_risk_vessels <- predictions_figures %>%
    filter(class ==1) %>%
    .$mmsi_anonymous %>% unique() %>% length()

# What is the fraction of unique vessels that were high-risk during at least 1 year?
fraction_high_risk_vessels <- number_high_risk_vessels /
  (predictions_figures %>%
    .$mmsi_anonymous %>% unique() %>% length())

# What is the number of potentially affected crew that were on high-risk vessels?
number_affected_crew <- predictions_figures %>%
  filter(class ==1) %>%
  group_by(mmsi_anonymous) %>%
  summarize(mean_crew = mean(crew_size,na.rm=TRUE)) %>%
  ungroup() %>%
  .$mean_crew %>%
  sum()

n_predicted_positive_vessel_years <- predictions_figures %>%
  filter(Prediction == "Positive") %>%
  nrow()

n_predicted_positive_vessel_years_new <- predictions_figures %>%
  filter(Prediction == "Positive" & Label == "Unlabeled") %>%
  nrow()

fraction_positive_vessel_years <- n_predicted_positive_vessel_years / nrow(predictions_figures)

correct_positives <- predictions_figures %>%
  filter(Prediction == "Positive" & Label == "Positive") %>%
  nrow()

fraction_correct_positives <- correct_positives / (predictions_figures %>%
  filter( Label == "Positive") %>%
  nrow())

riskiest_gears <- predictions_figures %>%
  group_by(gear,class) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  pivot_wider(names_from = "class",values_from="count") %>%
  mutate(fraction_risky = `1` / (`1` + `0`),
         percentage_risky = round(fraction_risky * 100))

riskiest_fleets <-predictions_figures  %>%
  filter(class == 1) %>%
  left_join(flag_predictions_other,by=c("Country","gear")) %>%
  mutate(Country = ifelse(Other,"Other",Country))%>%
  group_by(mmsi_anonymous,Country,gear,year) %>%
  summarize(crew_size = mean(crew_size)) %>%
  ungroup() %>%
  group_by(Country,gear) %>%
  summarize(`Number of vessels` = n(),
            `Number of crew` = sum(crew_size)) %>%
  ungroup() %>%
  gather("indicator","count",-gear,-Country) %>%
  mutate(indicator = fct_relevel(indicator,c("Number of vessels","Number of crew"))) %>%
  filter(indicator == "Number of vessels") %>%
  arrange(desc(count)) %>%
  slice(1:5) %>%
  mutate(gear = case_when(gear == "squid_jigger" ~ "squid jiggers",
                                    gear == "drifting_longlines" ~ "longliners",
                                    gear == "trawlers" ~ "trawlers")) %>% 
  mutate(statement = paste(Country, gear)) %>% 
  .$statement %>% 
  paste(collapse = ", ")
```

```{r eval = TRUE}
## Port statistics
# How many countries had visits by known offenders in the 2012-2018 timeframe?
port_countries_known_offenders <- sum(port_visit_summary$has_known_offender_visits)

# How many countries have high-risk visits in 2018, but no known offender visits
port_countries_high_risk_no_known_offenders <- port_data %>%
  filter(high_risk_visits>0 & no_known_offender_visits) %>%
  .$port_iso3 %>%
  unique() %>%
  length()

# Which countries have high-risk visits in 2018
countries_at_risk <- port_data %>% filter(fraction_high_risk_visits >0) %>% 
  mutate(country = countrycode(port_iso3,"iso3c","country.name")) %>% distinct(country) %>% arrange(country)

# How many countries had high-risk visits in 2018
n_countries_at_risk <-  nrow(countries_at_risk)

# What is the fraction of all countries that had high-risk port visits in 2018
fraction_countries_high_risk_port_visits <- n_countries_at_risk / (port_data %>% filter(!is.na(gear)) %>% .$port_iso3 %>% unique() %>% length())

# Which countries that had ratified the PSMA had high-risk port visits in 2018
psma_countries_at_risk <- port_data %>% 
  filter(psma_country & fraction_high_risk_visits >0) %>% 
  mutate(country = countrycode(port_iso3,"iso3c","country.name")) %>% 
  distinct(country) %>% 
  arrange(country)

# How many countries that had ratified the PSMA had high-risk port visits in 2018
number_psma_countries_at_risk <- psma_countries_at_risk%>% 
  nrow()

```

Our PU approach leverages information from all positively labeled vessels (n = `r training_df %>% group_by(offender) %>% summarize(count = n_distinct(mmsi)) %>% filter(offender==1) %>% .$count` unique vessels across `r training_df %>% group_by(offender) %>% summarize(count = n()) %>% filter(offender==1) %>% .$count %>% prettyNum(big.mark=',')` vessel-years, using our base model assumption), but places less emphasis on unlabeled vessels given their uncertain nature (n = `r training_df %>% group_by(offender) %>% summarize(count = n_distinct(mmsi)) %>% filter(offender==0) %>% .$count` unique vessels across `r training_df %>% group_by(offender) %>% summarize(count = n()) %>% filter(offender==0) %>% .$count %>% prettyNum(big.mark=',')` vessel-years, using our base model assumption).

We train the predictive model using vessel monitoring data from Global Fishing Watch . For `r prettyNum(prediction_df %>% summarize(count = n_distinct(mmsi)) %>% .$count, big.mark=',')` unique longliner, trawler, and squid jiggers vessel vessels, we calculate a number of features on an annual basis from 2012 - 2018 (SI Table S1). We call the unit of observation a â€œvessel-yearâ€. These features represent aggregate annual observable vessel behavior features. We also include vessel characteristic features such as vessel flag and engine power. This training dataset includes `r prettyNum(nrow(prediction_df),big.mark=',')` vessel-years of observation. 

The model identifies `r prettyNum(n_predicted_positive_vessel_years_new %>% signif(2),big.mark=",")` new high-risk vessel-years that were previously unlabeled.

The model correctly identifies `r round(fraction_correct_positives * 100)`% of positive vessel-years as being high-risk, while also identifying `r prettyNum(n_predicted_positive_vessel_years %>% signif(2),big.mark=",")` total high-risk vessel-years (`r round(fraction_positive_vessel_years * 100)`% of the total vessel-years). 

`r prettyNum(number_high_risk_vessels %>% signif(2),big.mark=",")` unique vessels were high-risk during at least one year (`r round(fraction_high_risk_vessels * 100)`% of the total unique vessels).

`r prettyNum(number_affected_crew %>% signif(2),big.mark=",")` crew members were working on these boats and thus potential victims of forced labor during at least one year. 

`r riskiest_fleets` represent the five fisheries with the largest number of unique high-risk vessels.

While longliners have the largest number of high-risk vessel-years across years, squid jiggers have the highest percentage of high-risk vessels across all years (`r riskiest_gears %>% filter(gear=="squid_jigger") %>% .$percentage_risky`%), followed by longliners (`r riskiest_gears %>% filter(gear=="drifting_longlines") %>% .$percentage_risky`%) and trawlers (`r riskiest_gears %>% filter(gear=="trawlers") %>% .$percentage_risky`%). 

We also find that known positive vessels visited ports in `r port_countries_known_offenders` countries during the 2012-2018-time frame using our base model assumptions. 

In 2018 alone, model-identified high-risk vessels visited ports across `r n_countries_at_risk` developed and developing countries in 2018 (`r round(fraction_countries_high_risk_port_visits*100)`% of all visited countries for these gear types), including `r number_psma_countries_at_risk` Parties to the Port State Measures Agreement (Figure 4). The visited ports are predominantly in Asia, Africa, and South America, with notable exceptions being Canada, United States, New Zealand, and several European countries. 

`r port_countries_high_risk_no_known_offenders` of the countries visited by high-risk vessels in 2018 had not been visited by known positive vessels, which is reflective of our limited training data set but may also be reflective of the limited port oversight currently occurring in many countries. 

In 2018, `r still_in_operation_stats %>% filter(year==2018) %>% .$count_in_operation` of `r predictions_figures %>% filter(!is.na(fldb_vessel_id)) %>% .$fldb_vessel_id %>% unique() %>% length()` (`r round((still_in_operation_stats %>% filter(year==2018) %>% .$fraction_in_operation)*100)`%) vessels with reported forced labor were still operating, with `r still_in_operation_stats %>% filter(year==2018) %>% .$count_high_risk` of those `r still_in_operation_stats %>% filter(year==2018) %>% .$count_in_operation` vessels (`r round((still_in_operation_stats %>% filter(year==2018) %>% .$fraction_high_risk)*100)`%) being classified as high-risk. 

```{r eval = TRUE, results='asis'}
countries_at_risk %>% 
  kableExtra::kbl(caption = "Countries that were visited by high-risk vessels in 2018",longtable=TRUE)
```

```{r eval = TRUE, results='asis'}
psma_countries_at_risk %>% 
  kableExtra::kbl(caption = "Countries that had ratified the PSMA and were visited by high-risk vessels in 2018",longtable=TRUE)
```

For the training dataset used in this analysis, `r round(known_vessel_info$known_length_m / known_vessel_info$total_vessel_years*100)`% of vessel-years have known vessel length from registries, `r round(known_vessel_info$known_tonnage_gt / known_vessel_info$total_vessel_years*100)`% have known gross tonnage, `r round(known_vessel_info$known_engine_power_kw / known_vessel_info$total_vessel_years*100)`% have known engine power, and `r round(known_vessel_info$known_crew / known_vessel_info$total_vessel_years*100)`% have known crew size.

## Using range of results from robustness checks

```{r eval = TRUE}
robustness_summary <- robustness_final %>%
  filter(include_vessel_characteristics) %>%
  pivot_longer(-c(year_assumption,include_vessel_characteristics)) %>%
  group_by(name) %>%
  mutate(value = signif(value,2)) %>%
  summarize(min = min(value,na.rm=TRUE),
            max = max(value,na.rm=TRUE)) %>%
  ungroup() %>%
  mutate(range = ifelse(!str_detect(name,"fraction"),
                        paste0(prettyNum(round(min),big.mark=",",scientific=FALSE),
                               " and ",
                               prettyNum(round(max),big.mark=",",scientific=FALSE)),
                        paste0(round(min*100),
                               "% and ",
                               round(max*100),"%")))

robustness_gears_summary <- robustness_gears %>%
  filter(include_vessel_characteristics)%>%
  group_by(gear) %>%
  summarize(min = min(fraction_risky,na.rm=TRUE),
            max = max(fraction_risky,na.rm=TRUE)) %>%
  ungroup() %>%
  mutate(range = paste0(round(min*100),
                               "% and ",
                               round(max*100),"%"))

robustness_fleets_summary <- robustness_fleets%>%
  filter(include_vessel_characteristics) %>%
  group_by(Country,gear) %>%
  summarize(count = sum(count)) %>%
  ungroup() %>%
  arrange(desc(count))%>%
  slice(1:5) %>%
  mutate(gear = case_when(gear == "squid_jigger" ~ "squid jiggers",
                                    gear == "drifting_longlines" ~ "longliners",
                                    gear == "trawlers" ~ "trawlers")) %>% 
  mutate(statement = paste(Country, gear)) %>% 
  .$statement %>% 
  paste(collapse = ", ")
```

The model correctly identifies between `r robustness_summary %>% filter(name=="fraction_correct_positives") %>% .$range` of positive vessel-years as being high-risk, while also identifying between `r robustness_summary %>% filter(name=="n_predicted_positive_vessel_years") %>% .$range` total high-risk vessel-years (between `r robustness_summary %>% filter(name=="fraction_positive_vessel_years") %>% .$range` of the total vessel-years). 

Between `r robustness_summary %>% filter(name=="n_predicted_positive_vessels") %>% .$range` unique vessels were high-risk during at least one year (between `r robustness_summary %>% filter(name=="fraction_positive_vessels") %>% .$range` of the total unique vessels).

Between `r robustness_summary %>% filter(name=="n_positive_crew") %>% .$range` crew members were working on these boats and thus potential victims of forced labor during at least one year. 

Looking across all model assumptions, `r robustness_fleets_summary` represent the five fisheries with the largest number of unique high-risk vessels.

While longliners have the largest number of high-risk vessel-years across years, squid jiggers have the highest percentage of high-risk vessels across all years (between `r robustness_gears_summary %>% filter(gear=="squid_jigger") %>% .$range`), followed by longliners (between `r robustness_gears_summary %>% filter(gear=="drifting_longlines") %>% .$range`) and trawlers (between `r robustness_gears_summary %>% filter(gear=="trawlers") %>% .$range`). 