---
title: "Forced labor final model building and predictions"
author: "Gavin McDonald - Sustainable Fisheries Group"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: 
  pdf_document: 
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.height=9.5, fig.width=7.5)
```

```{r}
set.seed(101)
# Necessary libraries
library(tidyverse)
library(tidymodels)
library(tictoc)
library(workflows)
library(countrycode)
library(bigrquery)
library(sf)
library(here)
library(rworldmap)
library(cowplot)
# Load custom threshold_perf function, adapted from probably::threshold_perf to give different metrics
source(here::here("r/threshold_perf_custom.R"))

# Set data directory based on system
# This references the emLab shared Team Drive
data_directory <- ifelse(Sys.info()["sysname"]=="Windows","G:/Shared\ drives/emLab/Projects/current-projects/forced-labor-and-fisheries/data/phase-1",
                         "/Volumes/GoogleDrive/Shared\ drives/emLab/Projects/current-projects/forced-labor-and-fisheries/data/phase-1")

# Read in cached data
training_df <- read_csv(here::here("raw_data/s1_training_full.csv")) %>%
  filter(!is.na(flag)) %>%
  # Only include gears for which we have known offenders
  filter(gear %in% c("drifting_longlines","squid_jigger","trawlers")) %>%
  dplyr::select(-years_until_caught) %>%
  # Add unique id for every row
  mutate(unique_id = row_number()) %>%
  # Make binaries into 1s/0s
  mutate(offender = ifelse(offender,1,0),
         actual = offender,
         foc = ifelse(foc,1,0),
         iuu = ifelse(iuu,1,0)) %>%
  # Add media source id
  # If it's not a known offender, create dummy ID for CV
  mutate(source_id = ifelse(is.na(source_id),
                            paste0("no_source_",row_number()),
                            source_id)) %>%
  # Add forced labor database vessel id
  # If it's not a known offender, create dummy ID for CV
  mutate(fldb_vessel_id = ifelse(is.na(fldb_vessel_id),
                            paste0("no_fldb_info_",row_number()),
                            fldb_vessel_id)) %>%
  # Make these columns factors
  mutate_at(
     vars("foc", "iuu", "gear", "flag", "year", "ais_type", "mmsi", "fldb_vessel_id", "year", "source_id", "offender","actual","unique_id"),
     list(as.factor))

# Bigquery project
ucsb_project <- "ucsb-gfw"
# Use GFW account, but only for this project
gfw_project <- "world-fishing-827"
```

```{r eval = FALSE}
# Recipe for processing data within each sampling iteration
fl_rec <- recipes::recipe(offender ~ ., data = head(training_df)) %>%
  recipes::update_role(mmsi, new_role = "id variable") %>%
  recipes::update_role(fldb_vessel_id, new_role = "id variable") %>%
  recipes::update_role(source_id, new_role = "id variable") %>%
  recipes::update_role(unique_id, new_role = "id variable") %>%
  recipes::update_role(actual, new_role = "id variable") %>%
  # Downsample unlabaled vessels
  # Skip=TRUE so that the testing dataset is not downsampled when baking
  # Do this at random every time
  #recipes::step_downsample("offender",under_ratio = tune(),skip=TRUE)  %>%
  recipes::step_knnimpute(ais_type,impute_with = c("gear","flag","length_m")) %>%
  # Box-Cox transformation on all numeric
  recipes::step_BoxCox(all_numeric()) %>%
  # Group flags with less than 1% into other
  recipes::step_other(flag, threshold = 0.01) %>%
  # Create dummy variables for factor columns like flag, ais_type
  recipes::step_dummy(all_predictors(), -all_numeric()) %>%
  # Remove near-zero variance numeric predictors
  recipes::step_nzv(all_predictors()) %>%
  # Remove numeric predictors that have correlation greater the 75%
  recipes::step_corr(all_numeric(), threshold = 0.75) %>%
  # Center all numeric predictors
  recipes::step_center(all_numeric()) %>%
  # Scale all numeric predictors
  recipes::step_scale(all_numeric())
```

```{r eval = FALSE}
# Define model specs
# Random forest
# Use default hyperparameters
rf_mod <- rand_forest(
  mode = "classification",
  trees = 1000
) %>%
  set_engine("ranger")

# SVM
# Use default hyperparameters
svm_mod <- svm_rbf(mode = "classification") %>%
  set_engine("kernlab",scaled=FALSE)
```

```{r eval = FALSE}
# Cross-validation
# Ensure there is no splitting across source_id (and consequently fldb_vessel_id) across analysis and assessment datasets
set.seed(101)
cv_splits <- group_vfold_cv(training_df, 
                            group = source_id,
                            v = 10)

# NA means no downsampling, so just the regular base classifier
under_ratio_vec <- tibble(under_ratio = c(NA,seq(1,5)))
# Number of bags
bag_vec <- tibble(bag = seq(100))
# Different models to try
model_type_vec <- tibble(model_type = c("random-forest","svm"))

analysis_data <- cv_splits %>%
  mutate(# Create analysis dataset based on CV folds
    analysis = map(splits,~analysis(.x)),
    # Create assessment dataset based on CV folds
    assessment = map(splits,~assessment(.x))) %>%
  dplyr::select(-splits)

model_runs <- crossing(under_ratio_vec,
                       model_type_vec,
           bag_vec) %>%
  # If there's no downsampling, there's no bagging
  mutate(bagging = ifelse(is.na(under_ratio),FALSE,TRUE)) %>%
  # Don't need to bag if not doing downsampling
  filter(!(!bagging & bag >1)) %>%
  # Want different seed each time downsampling is done, for bagging
  mutate(seed = sample.int(10^5,n()))

pb <- progress_estimated(nrow(model_runs) * nrow(cv_splits))

predictions <- model_runs %>%
  mutate(recipe = map2(seed,bagging,function(x,y){
    # Add downsampling to pre-processing recipe if bagging
    ifelse(!y,
           tmp_recipe <- fl_rec,
           tmp_recipe <- fl_rec %>%
             step_downsample("offender",under_ratio=y,seed=x,skip=TRUE))
    return(tmp_recipe)
    # Create workflow based on random forest hyperparameter and downsampling pre-processing recipe
  }),
  workflow = map2(recipe,model_type,function(x,y){
    # Add either random forest or svm to worflow, along with appropriate recipe
    ifelse(y == "random-forest",
           tmp_workflow <- workflow() %>%
             add_model(rf_mod) %>%
             add_recipe(x),
           tmp_workflow <- workflow() %>%
             add_model(svm_mod) %>%
             add_recipe(x))
    return(tmp_workflow)
  })) %>%
  # Remove unnecessary columns
  dplyr::select(-seed,-recipe) %>%
  # For each workflow, do entire model fitting process across all folds
  mutate(predict = map(workflow,function(x){
    analysis_data %>%
      mutate(predict = map2(analysis,assessment,function(y,z){
        tmp <- fit(x,y) %>%
          # Predict assessment data using fit
          predict(z, type = "prob") %>%
          # Add predictions to assessment data
          bind_cols(z) %>%
          # Select relevant columns
          dplyr::select(mmsi,year,offender,.pred_1)
        # Increment progress bar
        pb$tick()$print()
        return(tmp)
      })) %>%
      dplyr::select(id,predict) %>%
      unnest(predict)
  })) %>%
  dplyr::select(-workflow) %>%
  unnest(predict)

write_csv(predictions,paste0(data_directory,"/tidy_cv_predictions_100.csv"))
```

```{r}
# Load cached predictions
predictions <- read_csv(paste0(data_directory,"/tidy_cv_predictions.csv"))
```

```{r eval = FALSE}
bags_to_show <- c(1,10,50,100)
bag_performance <- map_df(bags_to_show,function(x){
  tmp_performance <- predictions %>%
    filter(bag<=x) %>%
    # For each fold and hyperparameter set, take average prediction for each vessel-year (mmsi and year) across bags
    group_by(id,model_type,under_ratio,mmsi,year) %>%
    summarize(offender = offender[1],
              .pred_1 = mean(.pred_1,na.rm=TRUE)) %>%
    ungroup()  %>%
  # Make offender =1 reference level
  mutate(offender = relevel(as.factor(offender),"1")) %>%
    # Then we will calculate performance metrics, by threshold, for each fold and hyperparameter set
    group_by(id,model_type,under_ratio) %>%
    nest() %>%
    ungroup() %>%
    # Calculated metrics will include recall, modified_f1, and detection_prevalence
    mutate(threshold_perf = map(data,~threshold_perf_custom(.x,
                                                            truth = offender,
                                                            estimate = .pred_1,
                                                            thresholds=seq(0.01,0.99,0.01)))) %>%
    dplyr::select(-data) %>%
    unnest(threshold_perf) %>%
    # Now for each hyperparameter set, we will take the average performance across all folds
    group_by(model_type,under_ratio,.threshold,.metric) %>%
    summarize(mean_performance = mean(.estimate,na.rm=TRUE),
              sd_performance = sd(.estimate,na.rm=TRUE)) %>%
    ungroup() %>%
    group_by(model_type,under_ratio) %>%
    nest() %>%
    ungroup() %>%
    # Now we will find the optimial threshold for maximizing the modified F1 estimator
    # Maximize the mean
    mutate(optimal_threshold = map_dbl(data,~.x %>%
                                         filter(.metric=="modified_f1") %>%
                                         arrange(desc(mean_performance)) %>%
                                         slice(1) %>%
                                         .$.threshold),
           # optimized_metrics will contain metrics with optimized modified F1 estimator
           optimized_metrics = map2(data,optimal_threshold,~.x %>%
                                      filter(.threshold==.y))) %>% 
    unnest(optimized_metrics) %>%
    dplyr::select(-optimal_threshold,-data) %>%
    mutate(number_bags = x)
  print(paste(x,"complete"))
  tmp_performance
})
# Save bag performance estimates for later
write_csv(bag_performance,paste0(data_directory,"/tidy_cv_bag_performance_100.csv"))
write_csv(bag_performance,here::here("interim_data/s7_figure_s3_data.csv"))
```

```{r}
# Read in cached bag performance
bag_performance <- read_csv(here::here("interim_data/s7_figure_s3_data.csv"))
cv_performance_fig <- bag_performance %>%
  filter(!is.na(under_ratio) | number_bags ==1) %>%
  mutate(number_bags = paste0("Bags: ",number_bags) %>%
           fct_relevel(paste0("Bags: ",bags_to_show))) %>%
  mutate(under_ratio = ifelse(is.na(under_ratio),"B",under_ratio) %>%
           fct_relevel("B")) %>%
  mutate(model_type=ifelse(model_type=="random-forest","Random forest","Support vector\nmachine")) %>%
  mutate(error_min = pmax(mean_performance-sd_performance,0),
         error_max = ifelse(.metric=="recall",pmin(mean_performance+sd_performance,1),mean_performance+sd_performance)) %>%
#  filter(.metric %in% c("recall","modified_f1")) %>% 
    mutate(.metric = case_when(.metric=="recall"~"Recall",
                               .metric=="modified_f1"~"Modified F1",
                               TRUE~"Detection\nprevalence")) %>%
  mutate(.metric = fct_relevel(.metric,"Recall","Detection\nprevalence")) %>%
  ggplot(aes(x = factor(under_ratio),y=mean_performance,color=model_type)) + 
  geom_point(position=position_dodge(width=0.75)) +
  geom_errorbar(aes(ymin=error_min, ymax=error_max),position=position_dodge(width=0.75),width=0.5) + 
  facet_grid(.metric~number_bags,scales="free_y",switch="y") +
  labs(x = "Downsampling ratio",
       y = "Mean\nacross\nfolds")+ 
  theme_bw() +
  theme(strip.background =element_rect(fill=NA),
        axis.title.y = element_text(angle=0,vjust=0.5),
        strip.text.y.left = element_text(angle=0,vjust=0.5)) +
  disco::scale_color_disco("Model type")

cv_performance_fig

ggsave(here::here("output_figures/figure_s3.png"),cv_performance_fig,width=7.5,height=7.5,device="png",dpi=300)

# Non-bagging version has standard error too large
# Stick with 5 bags to reduce variation
# Stick with under_ratio of 1 for simplest model
# Random forest has lowest standard deviation for modified f1, and has a higher recall
optimized_model <- tibble(model_type = "random-forest",
                          bagging = TRUE,
                          under_ratio = 1,
                          number_bags = 100)

cv_performance <- bag_performance %>% 
  mutate(bagging = ifelse(is.na(under_ratio),FALSE,TRUE)) %>%
  inner_join(optimized_model,by = c("model_type", "number_bags", "under_ratio","bagging"))

optimized_model$.threshold <- cv_performance$.threshold[1]
```

```{r eval = FALSE}
# Train final model on full dataset
set.seed(101)
# Define all model runs
model_runs_final <- optimized_model %>%
  crossing(tibble(bag = seq(optimized_model$number_bags))) %>%
  mutate(seed = sample.int(10^5,n()))

pb <- progress_estimated(nrow(model_runs_final))

predictions_final <- model_runs_final %>%
  mutate(recipe = map2(seed,bagging,function(x,y){
    # Add downsampling to pre-processing recipe if bagging
    ifelse(!y,
           tmp_recipe <- fl_rec,
           tmp_recipe <- fl_rec %>%
             step_downsample("offender",under_ratio=y,seed=x,skip=TRUE))
    return(tmp_recipe)
    # Create workflow based on random forest hyperparameter and downsampling pre-processing recipe
  }),
  workflow = map2(recipe,model_type,function(x,y){
    # Add either random forest or svm to worflow, along with appropriate recipe
    ifelse(y == "random-forest",
           tmp_workflow <- workflow() %>%
             add_model(rf_mod %>%
             set_args(importance = "impurity_corrected")) %>%
             add_recipe(x),
           tmp_workflow <- workflow() %>%
             add_model(svm_mod) %>%
             add_recipe(x))
    return(tmp_workflow)
  })) %>%
  # Remove unnecessary columns
  dplyr::select(-seed,-recipe) %>%
  # For each workflow, do entire model fitting process across all folds
  mutate(fit = map(workflow,~fit(.x,training_df)),
         predict = map(fit,function(x){
           tmp <- x %>%
             # Predict assessment data using fit
             predict(training_df, type = "prob") %>%
             # Add predictions to assessment data
             bind_cols(training_df)%>%
             # Select relevant columns
             dplyr::select(mmsi,year,offender,.pred_1)
           # Increment progress bar
           pb$tick()$print()
           return(tmp)
         })) %>%
  dplyr::select(-workflow)

saveRDS(predictions_final,paste0(data_directory,"/predictions_final.Rdata"))

as.numeric.factor <- function(x) {as.numeric(levels(x))[x]}
predictions_figures <- predictions_final %>%
  unnest(predict) %>%
  # Switch factors to numeric for faster computation, then switch back
  mutate(offender = as.numeric.factor(offender),
         mmsi = as.numeric.factor(mmsi),
         year = as.numeric.factor(year)) %>%
    # Take average prediction for each vessel-year (mmsi and year) across bags
    group_by(mmsi,year) %>%
    summarize(offender = offender[1],
              .pred_1 = mean(.pred_1,na.rm=TRUE)) %>%
    ungroup() %>% 
  mutate(mmsi = as.factor(mmsi),
         year = as.factor(year)) %>%
  left_join(training_df %>%
              dplyr::select(-offender),
            by=c("mmsi","year")) %>%
  mutate(Country = countrycode(flag,"iso3c","country.name"))%>%
  # Use optimal threshold found above
  mutate(class = ifelse(.pred_1 >optimized_model$.threshold,1,0))%>% 
  mutate(Prediction = ifelse(class == 1,"Positive","Negative")) %>%
  mutate(Label = ifelse(offender == 0,"Unlabeled","Positive"))

# Save final predictions
write_csv(predictions_figures,here::here("interim_data/s4_final_model_predictions.csv"))

vessel_lookup <- read_csv(here::here("raw_data/s2_indicator_lookup.csv"))

# Get average variable importance across bags

importance_data <- predictions_final %>%
  mutate(var_imp = map(fit,function(x){
    x$fit$fit$fit$variable.importance%>%
          as.data.frame() %>%
          rownames_to_column() %>%
          `colnames<-`(c("Feature", "Importance"))
  })) %>%
  dplyr::select(bag,var_imp) %>%
  unnest(var_imp) %>%
  group_by(Feature) %>%
  summarize(Importance=mean(Importance,na.rm=TRUE)) %>%
  ungroup() %>%
  left_join(vessel_lookup %>%
              rename(Feature = indicator),by="Feature") %>%
  mutate(indicator_name = case_when(Feature == "gear_squid_jigger" ~ "Squid jigger",
                                    Feature == "gear_drifting_longlines" ~ "Longliner",
                                    Feature == "gear_trawlers" ~ "Trawler",
                                    Feature == "ais_type_A" ~ "AIS device type A",
                                    Feature == "ais_type_B" ~ "AIS device type B",
                                    Feature == "flag_other" ~ "Flag: Other",
                                    str_detect(Feature,"year") ~ paste0("Year: ", str_extract(Feature,"(\\d)+")),
                                    str_detect(Feature,"flag") ~ paste0("Flag: ", countrycode(word(Feature, 2, sep = "_"),"iso3c","country.name")),
                                    TRUE ~ indicator_name)) 
# Save data for figure 4
write_csv(importance_data,here::here("interim_data/s8_figure_s5_data.csv"))
```

# Make figures

```{r}
predictions_figures <- read_csv(here::here("interim_data/s4_final_model_predictions.csv"))
```

## Variable importance
```{r}
importance_fig <- read_csv(here::here("interim_data/s8_figure_s5_data.csv")) %>%
  ggplot(aes(x = reorder(indicator_name,Importance), y = Importance)) +
  geom_bar(stat = "identity",color="black") +
  coord_flip() +
  labs(x = "",
       y = "Mean importance across bags") +
  theme_bw() +
  theme(panel.grid.minor = element_blank())

importance_fig

ggsave(here::here("output_figures/figure_s5.png"),importance_fig,width=7.5,height=5,device="png",dpi=300)
```

## Classification by training label

```{r}
n_fun <- function(x){
  return(data.frame(y = max(x) + 0.05, label = paste0("n = ",prettyNum(length(x),big.mark=","))))
}

class_fig <- predictions_figures  %>%
  ggplot(aes(x=Prediction,y=.pred_1,fill=Label)) + 
  geom_boxplot(position = position_dodge(width = 0.75)) + 
  coord_flip() + 
  geom_hline(yintercept = optimized_model$.threshold,linetype=2) +
  labs(y = "Model risk score",
       x = "Model\nclassification")+
    stat_summary(fun.data = n_fun, geom = "label",position = position_dodge(width = 0.75),show.legend = FALSE) +
  ylim(c(0,1)) +
  disco::scale_fill_disco(palette = "vibrant", "Training data\nlabel",direction=1,alpha=1,
                           guide = guide_legend(reverse = TRUE) ) +
  theme_bw() +
  theme(axis.title.y = element_text(angle=0,vjust=0.5))

class_fig

ggsave(here::here("output_figures/figure_s4.png"),class_fig,width=7.5,height=3.5,device="png",dpi=300)
```

## High risk vessels and crew over time

```{r}
# What is the number of unique high-risk vessels (vessels that were high-risk during at least 1 year)?
number_high_risk_vessels <- predictions_figures %>%
    filter(class ==1) %>%
    .$mmsi %>% unique() %>% length()

# What is the fraction of unique vessels that were high-risk during at least 1 year?
fraction_high_risk_vessels <- number_high_risk_vessels /
  (predictions_figures %>%
    .$mmsi %>% unique() %>% length())

# What is the number of potentially affected crew that were on high-risk vessels?
number_affected_crew <- predictions_figures %>%
  filter(class ==1) %>%
  group_by(mmsi) %>%
  summarize(mean_crew = mean(crew_size,na.rm=TRUE)) %>%
  ungroup() %>%
  .$mean_crew %>%
  sum()

flag_predictions_other <- predictions_figures %>%
  filter(fishing_hours > 0) %>%
  #filter(year == 2018) %>%
  filter(class == 1) %>%
  group_by(Country,gear) %>%
  summarize(count_vessels = n_distinct(mmsi)) %>%
  ungroup() %>%
  group_by(gear) %>%
  mutate(fraction = count_vessels / sum(count_vessels)) %>%
  mutate(Other = ifelse(fraction<0.025,TRUE,FALSE))

vessels_fig_total <- predictions_figures  %>%
  filter(fishing_hours > 0) %>%
  #filter(year == 2018) %>%
  filter(class == 1) %>%
  left_join(flag_predictions_other,by=c("Country","gear")) %>%
  mutate(Country = ifelse(Other,"Other",Country))%>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "Longliner",
                          gear=="squid_jigger" ~ "Squid jigger",
                          TRUE ~ "Trawler")) %>%
  group_by(Country,gear,year) %>%
  summarize(`Number of vessels` = n()) %>%
  ungroup() %>%
  ggplot(aes(x = year, y = `Number of vessels`, fill = Country)) +
  geom_bar(stat = "identity", color = "black",size=0.25) +
  #coord_flip() +
  labs(x="",title="Number of high-risk vessels",y="") +
  theme_bw() +
  disco::scale_fill_disco(palette = "rainbow", "Flag") +
  facet_grid(.~gear)+
  scale_y_continuous(labels=scales::comma)+ 
  scale_x_continuous(breaks = unique(predictions_figures$year)) +
  theme(strip.background =element_rect(fill=NA),
        axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.minor = element_blank())

vessels_fig_percentage <- predictions_figures %>%
  group_by(year,gear) %>%
  summarize(fraction_high_risk = sum(class) / n())%>%
  ungroup() %>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "Longliner",
                          gear=="squid_jigger" ~ "Squid jigger",
                          TRUE ~ "Trawler")) %>%
  ggplot(aes(x = year, y = fraction_high_risk)) +
  geom_bar(stat="identity") +
  facet_grid(.~gear) +
  theme_bw()+
  scale_y_continuous(labels=scales::percent,limits = c(0,1))+ 
  scale_x_continuous(breaks = unique(predictions_figures$year)) +
  theme(strip.background =element_rect(fill=NA),
        axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.minor = element_blank())+
  labs(x="",title="Percentage of total vessels that are high-risk",y="")

legend <- get_legend(
  # create some space to the left of the legend
  vessels_fig_total
)

vessels_fig <- cowplot::plot_grid(plot_grid(vessels_fig_total+ theme(legend.position="none"),
                             vessels_fig_percentage,
                             align = "v", 
                             ncol = 1),
                   legend,
                   ncol=2,
                   rel_widths = c(4,1))  

ggsave(here::here("output_figures/figure_2.png"),vessels_fig,width=7.5,height=5,device="png",dpi=300)
```




```{r}
training_df_figure <- read_csv(here::here("raw_data/s1_training_full.csv")) %>%
  filter(!is.na(flag)) %>%
  # Only include gears for which we have known offenders
  filter(gear %in% c("drifting_longlines","squid_jigger","trawlers")) %>%
  # Add unique id for every row
  mutate(unique_id = row_number()) %>%
  # Make binaries into 1s/0s
  mutate(offender = ifelse(offender,1,0),
         actual = offender,
         foc = ifelse(foc,1,0),
         iuu = ifelse(iuu,1,0)) %>%
  dplyr::select(-year,-flag,-actual,-source_id,-unique_id,-years_until_caught,-fldb_vessel_id) %>%
  mutate(ais_type = ifelse(ais_type == "A",1,0)) 

asinh_trans <- function(){
  scales::trans_new(name = 'asinh', transform = function(x) asinh(x), 
            inverse = function(x) sinh(x))
}

indicator_figure_data <- recipes::recipe(offender ~ ., data = head(training_df_figure)) %>%
  recipes::update_role(mmsi, new_role = "id variable") %>%
  recipes::update_role(gear, new_role = "id variable") %>%
  # Box-Cox transformation on all numeric
  recipes::step_BoxCox(all_numeric())%>%
  step_normalize(all_predictors()) %>%
  prep(training_df_figure) %>%
  juice() %>%
  pivot_longer(cols=c(-"mmsi",-"gear",-"offender"),names_to = "indicator") %>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "Longliner",
                          gear=="squid_jigger" ~ "Squid jigger",
                          TRUE ~ "Trawler")) %>%
    left_join(vessel_lookup,by="indicator") %>%
  mutate(offender=ifelse(offender==1,"Positive","Unlabeled")) 

indicator_figure <- indicator_figure_data %>%
  ggplot(aes(x = reorder(indicator_name,value),y=value,color=factor(offender))) +
  geom_boxplot(position=position_dodge(width=1),outlier.size = 0.1) +
  disco::scale_color_disco(palette = "vibrant", "Training data label",direction=1,alpha=1,
                           guide = guide_legend(reverse = TRUE) ) +
  coord_flip() +
  facet_grid(.~gear) +
  scale_y_continuous(trans="asinh",breaks = scales::pretty_breaks(2)) +
    labs(
      x = "",
      y = "Box-Cox transformed, centered, and scaled\n vales (inverse hyperbolic sine scale)"
    ) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1,size=8),
        legend.position = "bottom",
        legend.direction = "horizontal",
        strip.background =element_rect(fill=NA))

indicator_figure

ggsave(here::here("output_figures/figure_1.png"),indicator_figure,width=8,height=8,device="png",dpi=300)
```

```{r}
training_dataset_fig <- training_df  %>%
  mutate(offender = ifelse(offender==1,"Number of positive vessel-years","Number of unlabeled vessel-years")) %>%
  mutate(Country = countrycode(flag,"iso3c","country.name")) %>%
  group_by(gear,offender,year) %>% 
  summarize(count=n()) %>% 
  ungroup() %>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "Longliner",
                          gear=="squid_jigger" ~ "Squid jigger",
                          TRUE ~ "Trawler"))%>%
  ggplot(aes(x=year,y=count,fill=gear)) + 
  geom_bar(stat="identity",color="black",size=0.25) + 
  #coord_flip() + 
  facet_wrap(offender~.,scales="free_y",ncol=1)+
  disco::scale_fill_disco(palette = "bright", "Gear") +
  theme_bw() +
  labs(x="",y="")+
  scale_y_continuous(labels=scales::comma_format(accuracy=1))+ 
  theme(strip.background =element_rect(fill=NA,
                                       color = NA),
        strip.text = element_text(hjust=0),
        axis.title.y = element_text(angle=0,vjust=0.5))

training_dataset_fig

ggsave(here::here("output_figures/figure_s2.png"),training_dataset_fig,width=7.5,height=4.5,device="png",dpi=300)
```

\pagebreak

## High risk fishing effort maps - 2018
```{r eval = FALSE}
as.numeric.factor <- function(x) {as.numeric(levels(x))[x]}
# Upload risk predictions
bq_table(project = ucsb_project,table = "risk_predictions",dataset = "human_rights") %>%
  bq_table_upload(values = predictions_figures %>% dplyr::select(mmsi,year,offender,probability=.pred_1,class,gear) %>%
                    mutate(mmsi = as.numeric.factor(mmsi),
                           year = as.numeric.factor(year)),
                  write_disposition = "WRITE_TRUNCATE")

sql <- "#standardSQL
WITH
  fishing_info AS(
  SELECT
    mmsi,
    EXTRACT(YEAR
    FROM
      timestamp) year,
    FLOOR(lat / 0.25) * 0.25 + 0.125 lat_bin,
    FLOOR(lon / 0.25) * 0.25 + 0.125 lon_bin,
    SUM((CASE
          WHEN nnet_score2 = 1 AND NOT (distance_from_shore_m < 1000 AND speed < 1) THEN hours
        ELSE
        0
      END
        )) fishing_hours
  FROM
    `ucsb-gfw.human_rights.all_vessel_positions`
  GROUP BY
    mmsi,
    lat_bin,
    lon_bin,
    year),
  extra_offsetting_filter AS(
  SELECT
    CAST(ssvid AS INT64) mmsi,
    year,
    TRUE extra_offsetting
  FROM
    `ucsb-gfw.human_rights.extra_offsetting_filter` ),
  vessel_info AS(
  SELECT
    mmsi,
    year,
    gear,
    flag,
    engine_power_kw
  FROM
    `ucsb-gfw.human_rights.vessel_info_all`),
  risk AS(
  SELECT
    mmsi,
    year,
    class,
    offender
  FROM
    `ucsb-gfw.human_rights.risk_predictions`),
  joined AS(
  SELECT
    *
  FROM
    vessel_info
  LEFT JOIN
    fishing_info
  USING
    (mmsi,
      year)
  LEFT JOIN
    risk
  USING
    (mmsi,
      year)
  LEFT JOIN
    extra_offsetting_filter
  USING
    (mmsi,
      year)
  WHERE extra_offsetting IS NULL)
SELECT
  year,
  gear,
  flag,
  lat_bin,
  lon_bin,
  SUM(fishing_hours) fishing_hours,
  SUM(fishing_hours * offender) known_offender_fishing_hours,
  SUM(fishing_hours * class) at_risk_fishing_hours,
  SUM(fishing_hours * engine_power_kw) fishing_kW_hours,
  SUM(fishing_hours * offender * engine_power_kw) known_offender_fishing_kW_hours,
  SUM(fishing_hours * class * engine_power_kw) at_risk_fishing_kW_hours
FROM
  joined
GROUP BY
  year,
  gear,
  flag,
  lat_bin,
  lon_bin"

bq_project_query(gfw_project,sql, destination_table = bq_table(project = ucsb_project,table = "grouped_fishing_info_predicted",dataset = "human_rights"),use_legacy_sql = FALSE, allowLargeResults = TRUE,write_disposition = "WRITE_TRUNCATE")

sql <- "#standardSQL
  WITH base AS(
  SELECT
    year,
    gear,
    FLOOR(lat_bin / 0.5) * 0.5 + 0.25 lat_bin,
    FLOOR(lon_bin / 0.5) * 0.5 + 0.25 lon_bin,
    fishing_kW_hours,
    at_risk_fishing_kW_hours,
    known_offender_fishing_kW_hours
  FROM
    `ucsb-gfw.human_rights.grouped_fishing_info_predicted`
  WHERE
    fishing_kW_hours > 0)
SELECT
  year,
  gear,
  lat_bin,
  lon_bin,
  SUM(fishing_kW_hours) fishing_kW_hours,
  SUM(at_risk_fishing_kW_hours) at_risk_fishing_kW_hours,
  SUM(known_offender_fishing_kW_hours) known_offender_fishing_kW_hours,
  SUM(known_offender_fishing_kW_hours) / SUM(fishing_kW_hours) fraction_known_offender_fishing_kW_hours,
  SUM(at_risk_fishing_kW_hours) / SUM(fishing_kW_hours) fraction_at_risk_fishing_kW_hours
FROM
  base
WHERE
  gear IN('drifting_longlines',
    'trawlers',
    'squid_jigger')
GROUP BY
  year,
  gear,
  lat_bin,
  lon_bin"

grouped_messages <- bq_project_query(gfw_project, sql) %>%
  bq_table_download(max_results = Inf)

write_csv(grouped_messages %>%
            filter(year==2018) %>%
            dplyr::select(-known_offender_fishing_kW_hours,
                  -fraction_known_offender_fishing_kW_hours) ,path=here::here("interim_data/s5_figure_3_data.csv"))
```

```{r}
grouped_messages <- read.csv(here::here("interim_data/s5_figure_3_data.csv"))

world_land <- rworldmap::getMap(resolution = "low") %>%
  st_as_sf()

map_palette <- "inferno"
map_palette_begin <- 0.2
na_color <- viridis::viridis(n=5,option=map_palette,begin=map_palette_begin)[1]

pal <- c(viridis::inferno(n = 8))[c(3,5:8)]
pal <- c("midnightblue",viridis::inferno(n = 7)[c(3:7)])

map_figure_rel <-   grouped_messages %>%
  filter(!is.na(gear))%>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "(A) Longliner",
                          gear=="squid_jigger" ~ "(B) Squid jigger",
                          TRUE ~ "(C) Trawler")) %>%
  ggplot() +
  geom_tile(aes(x = lon_bin, y = lat_bin, fill = fraction_at_risk_fishing_kW_hours,color=fraction_at_risk_fishing_kW_hours)) +
  geom_sf(data = world_land, fill = "grey45", color = "grey45") +
  scale_fill_gradientn(name = "Percentage\nfishing kW-hours\nhigh-risk", na.value = pal[1], values = c(0,0.1,.25,0.5,0.75,1),labels=paste0(seq(0,1,.25)*100,"%"),breaks=seq(0,1,.25),colors=pal)  +
  theme_minimal() +
  theme(panel.background = element_rect(fill="black"),
        panel.grid.major = element_line(colour = "black"),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        text = element_text(size=15),
        legend.direction = "vertical"
  ) +
  labs(
    x = "",
    y = "")+ 
  ylim(c(-70, NA)) +
  scale_color_gradientn(name = "Percentage\nfishing kW-hours\nhigh-risk", na.value = pal[1], values = c(0,0.1,.25,0.5,0.75,1),guide=FALSE,labels=paste0(seq(0,1,.25)*100,"%"),breaks=seq(0,1,.25),colors=pal)  +
  facet_wrap(.~gear,ncol=1)

map_figure_rel
ggsave(here::here("output_figures/figure_3.png"),map_figure_rel,width=7.5,height=7.5,device="png",dpi=300)                                    
```

\pagebreak

## High risk port visit maps - 2018

```{r eval = FALSE}
## Port mapping
sql<-"#standardSQL
WITH
  anchorage_ids AS(
  SELECT
    s2id anchorage_id,
    iso3 port_iso3
  FROM
    `world-fishing-827.gfw_research.named_anchorages` ),
  port_visits AS(
  SELECT
    CAST(ssvid AS INT64) mmsi,
    EXTRACT(YEAR
    FROM
      trip_start) year,
    trip_start_anchorage_id anchorage_id
  FROM
    `world-fishing-827.gfw_research.vessel_voyages_v20190722`
  UNION ALL (
    SELECT
      CAST(ssvid AS INT64) mmsi,
      EXTRACT(YEAR
      FROM
        trip_start) year,
      trip_end_anchorage_id anchorage_id
    FROM
      `world-fishing-827.gfw_research.vessel_voyages_v20190722`) )
SELECT
  *
FROM
  port_visits
LEFT JOIN
  anchorage_ids
USING
  (anchorage_id)"

bq_project_query(gfw_project,sql, destination_table = bq_table(project = ucsb_project,table = "port_visits",dataset = "human_rights"),use_legacy_sql = FALSE, allowLargeResults = TRUE,write_disposition = "WRITE_TRUNCATE")

sql <- "WITH
  port_base AS(
  SELECT
    *
  FROM
    `ucsb-gfw.human_rights.port_visits`
  WHERE
    year < 2019),
  risk_predictions AS(
  SELECT
    class,
    gear,
    year,
    mmsi
  FROM
    `ucsb-gfw.human_rights.risk_predictions`),
  psma AS(
  SELECT
    iso3 port_iso3,
    EXTRACT(YEAR
    FROM
      date_poc) psma_year
  FROM
    `ucsb-gfw.human_rights.foc_poc_database` ),
  joined AS(
  SELECT
    *
  FROM
    port_base
  JOIN
    risk_predictions
  USING
    (mmsi,
      year)),
  final AS(
  SELECT
    year,
    gear,
    port_iso3,
    COUNT(*) total_visits,
    SUM(class) high_risk_visits,
    SUM(class) / COUNT(*) fraction_high_risk_visits
  FROM
    joined
  GROUP BY
    year,
    gear,
    port_iso3)
SELECT
  *,
IF
  (psma_year IS NULL
    OR year < psma_year,
    FALSE,
    TRUE) psma_country
FROM
  final
LEFT JOIN
  psma
USING
  (port_iso3)"

bq_project_query(gfw_project,sql, destination_table = bq_table(project = ucsb_project,table = "port_visits_risk_grouped",dataset = "human_rights"),use_legacy_sql = FALSE, allowLargeResults = TRUE,write_disposition = "WRITE_TRUNCATE")

sql <- "SELECT *
  FROM
    `ucsb-gfw.human_rights.port_visits_risk_grouped`"

port_data <- bq_project_query(gfw_project, sql) %>%
  bq_table_download(max_results = Inf)

write_csv(port_data %>%
            filter(year==2018),path=here::here("interim_data/s6_figure_4_data.csv"))
```

```{r}
port_data <- read_csv(here::here("interim_data/s6_figure_4_data.csv")) %>%
  filter(!is.na(port_iso3))

world_land_ports <- world_land %>% 
  as_tibble() %>%
  mutate(port_iso3 = as.character(ISO3.1)) %>%
  dplyr::select(geometry,port_iso3) %>%
  inner_join(port_data,by="port_iso3") %>%
  st_as_sf()

countries_at_risk <- port_data %>% filter(fraction_high_risk_visits >0) %>% .$port_iso3 %>% unique() %>% sort() %>% length()

countries <- length(unique(port_data$port_iso3))

fraction_countries_at_risk <- countries_at_risk / countries

psma_countries_at_risk <- port_data %>% filter(psma_country & fraction_high_risk_visits >0) %>% .$port_iso3 %>% unique() %>% sort()

number_psma_countries_at_risk <- psma_countries_at_risk%>% length()

pal <- c("midnightblue",viridis::inferno(n = 7)[c(3:7)])

port_map_figure_rel <-   world_land_ports %>%
  filter(!is.na(gear))%>%
  mutate(gear = case_when(gear=="drifting_longlines" ~ "(A) Longliner",
                          gear=="squid_jigger" ~ "(B) Squid jigger",
                          TRUE ~ "(C) Trawler")) %>%
  ggplot() +
  geom_sf(data = world_land, fill = "grey45", color = "grey45") +
  geom_sf(aes(
    fill = fraction_high_risk_visits
  ),
    color = "grey40",
    size=0.1) +
  scale_fill_gradientn(name = "Percentage\nport visits\nhigh-risk", na.value = pal[1], values = c(0,0.1,.25,0.5,0.75,1),labels=paste0(seq(0,1,.25)*100,"%"),breaks=seq(0,1,.25),colors=pal)  +
  theme_minimal() +
  theme(panel.background = element_rect(fill="black"),
        panel.grid.major = element_line(colour = "black"),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        text = element_text(size=15),
        legend.direction = "vertical"
  ) +
  labs(
    x = "",
    y = ""
  ) + 
  ylim(c(-70, NA)) +
  facet_wrap(.~gear,ncol=1)

#port_map_figure <- cowplot::plot_grid(port_map_figure_abs,port_map_figure_rel)
port_map_figure_rel

ggsave(here::here("/output_figures/figure_4.png"),port_map_figure_rel,width=7.5,height=7.5,device="png",dpi=300)      
```

## Paper stats
```{r}
n_predicted_positive_vessel_years <- predictions_figures %>%
  filter(Prediction == "Positive") %>%
  nrow()

n_predicted_positive_vessel_years_new <- predictions_figures %>%
  filter(Prediction == "Positive" & Label == "Unlabeled") %>%
  nrow()

fraction_positive_vessel_years <- n_predicted_positive_vessel_years / nrow(training_df)


n_predicted_positive_vessels <- predictions_figures %>%
  filter(Prediction == "Positive") %>%
  distinct(mmsi) %>%
  nrow()

fraction_positive_vessels <- n_predicted_positive_vessels / length(unique(predictions_figures$mmsi))

n_positive_crew <- predictions_figures %>%
  filter(Prediction == "Positive") %>%
  distinct(mmsi,crew_size) %>%
  .$crew_size %>%
  sum()

correct_positives <- predictions_figures %>%
  filter(Prediction == "Positive" & Label == "Positive") %>%
  nrow()

fraction_correct_positives <- correct_positives / (predictions_figures %>%
  filter( Label == "Positive") %>%
  nrow())

riskiest_fleets <-predictions_figures  %>%
  filter(fishing_hours > 0) %>%
  filter(class == 1) %>%
  left_join(flag_predictions_other,by=c("Country","gear")) %>%
  mutate(Country = ifelse(Other,"Other",Country))%>%
  group_by(mmsi,Country,gear,year) %>%
  summarize(crew_size = mean(crew_size)) %>%
  ungroup() %>%
  group_by(Country,gear) %>%
  summarize(`Number of vessels` = n(),
            `Number of crew` = sum(crew_size)) %>%
  ungroup() %>%
  gather("indicator","count",-gear,-Country) %>%
  mutate(indicator = fct_relevel(indicator,c("Number of vessels","Number of crew"))) %>%
  filter(indicator == "Number of vessels") %>%
  arrange(desc(count)) %>%
  slice(1:5)

```

# Time at sea stat for paper - fraction of total time at sea by included vessels

```{r eval = FALSE}
sql<-"WITH
included_vessels AS(
SELECT
year,mmsi,TRUE included
FROM
`ucsb-gfw.human_rights.risk_predictions`
),
base AS(
SELECT
  CAST(ssvid AS INT64) mmsi,year,
  activity.active_hours active_hours
FROM
  `world-fishing-827.gfw_research.vi_ssvid_byyear_v20190430`
  WHERE on_fishing_list_best),
joined AS(
 SELECT
mmsi,
year,
active_hours,
IF(included,TRUE,FALSE) included
FROM
base
LEFT JOIN
included_vessels
USING(mmsi,year)),
summary AS(
SELECT
SUM(active_hours) active_hours,
included
FROM joined
GROUP BY
included)
SELECT
(SELECT active_hours FROm summary WHERE included)/((SELECT active_hours FROm summary WHERE NOT included)+(SELECT active_hours FROm summary WHERE included)) fraction_active_hours_included"
```

```{r}
# Make figure summarizing ILO indicators
# Pull in forced labor vessel database
ilo_data <-read_csv(here::here("interim_data/s3_suspected_forced_labor_database.csv")) %>%
  # Filter to just those cases that were used in the analysis
  filter(fldb_vessel_id %in% training_df$fldb_vessel_id) %>%
  dplyr::select(fldb_vessel_id,forced_labor_indicator)%>%
  separate_rows(forced_labor_indicator,sep = ' \\| ') %>%
  mutate(forced_labor_indicator = case_when(forced_labor_indicator == "Abusive working and living conditons" ~ "Abusive working and living conditions",
                                            forced_labor_indicator == "Witholding of wages" ~ "Withholding of wages",
                                            TRUE ~ forced_labor_indicator)) %>%
  mutate(fldb_vessel_id = as.character(fldb_vessel_id))

# Summarize data by indicator, for organizing figure
ilo_summary_data <- ilo_data %>%
  group_by(forced_labor_indicator) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  arrange(count)
# Summarize data by vessel, for organizing figure
ilo_summary_vessel<- ilo_data %>%
  group_by(fldb_vessel_id) %>%
  summarize(count = n()) %>%
  ungroup() %>%
  arrange(-count)

ilo_indicator_fig <- ilo_data %>%
  mutate(forced_labor_indicator = fct_relevel(forced_labor_indicator,ilo_summary_data$forced_labor_indicator))%>%
  mutate(fldb_vessel_id = fct_relevel(fldb_vessel_id,ilo_summary_vessel$fldb_vessel_id)) %>%
  ggplot(aes(x=fldb_vessel_id,y=forced_labor_indicator)) +
  geom_count() +
  theme_bw() +
  theme(axis.text.x = element_blank(),
        legend.position = "none") +
  labs(x = "Case",y="") +
  coord_equal()

ilo_indicator_fig
ggsave(here::here("output_figures/figure_s1.png"),ilo_indicator_fig,width=7.5,height=3,device="png",dpi=300)

ilo_indicator_fig_2 <- ilo_summary_data %>%
  ggplot(aes(x = reorder(forced_labor_indicator,count), y = count)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "",
       y = "Number of cases\nexhibiting this ILO forced labor indicator") +
  theme_bw() +
  theme(panel.grid.minor = element_blank())
```


